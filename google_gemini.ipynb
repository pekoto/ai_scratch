{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04269a7a-f478-4583-bf08-deb13df1d0bf",
   "metadata": {},
   "source": [
    "# Google Gemini Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e8583-169c-4ced-bef5-f3eda750db61",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "* __gemini-pro__: Optimized for high intelligence tasks, the most powerful Gemini model\n",
    "* __gemini-flash__: Optimized for multi-modal use-cases, where speed and cost are important\n",
    "* __text-embedding__: Generates text embedding.\n",
    "* __aqa__: Perform Attributed Question-Answering (AQA)â€“related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability.\n",
    "\n",
    "[Ref](https://ai.google.dev/gemini-api/docs/models/gemini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de631d5b-06c8-4f21-98b5-6da129de3713",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "\n",
    "Use the pro model:\n",
    "\n",
    "```python\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# generate_content handle various use cases, including multimodal input\n",
    "response = model.generate_content(\"What is the meaning of life?\")\n",
    "\n",
    "# Responses re given in response.text\n",
    "# You can use a method to convert the output to markdown\n",
    "to_markdown(response.text)\n",
    "\n",
    "# You can use response.prompt_feedback to understand why there was no response (e.g. there may be safety concerns)\n",
    "response.prompt_feedback\n",
    "\n",
    "# You can view multiple possible responses with response.candidates\n",
    "response.candidates\n",
    "\n",
    "# Responses can also be streamed, instead of waiting for the whole thing to be generating at once.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0830da-ac65-4d4a-a86a-34c8c203e1f1",
   "metadata": {},
   "source": [
    "## Generating Text From Images and Text Inputs\n",
    "\n",
    "```python\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "img = PIL.Image.open('image.jpg')\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "response = model.generate_content(img)\n",
    "\n",
    "to_markdown(response.text)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "> This image shows two glass containers filled with prepared food...\n",
    "\n",
    "You can also pass in a list of strings and images:\n",
    "\n",
    "```python\n",
    "response = model.generate_content([\"Write a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.\", img], stream=True)\n",
    "\n",
    "response.resolve()\n",
    "\n",
    "to_markdown(response.text)\n",
    "```\n",
    "\n",
    "> Meal prepping is a great way to save time and money, and it can also help you to eat healthier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59e1e1-61f0-4709-b270-6d12c310595f",
   "metadata": {},
   "source": [
    "## Chat Conversations\n",
    "\n",
    "You can use the `ChatSession` class to manage conversation state.\n",
    "\n",
    "```python\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "chat = model.start_chat(history=[])\n",
    "chat\n",
    "```\n",
    "\n",
    "```\n",
    "ChatSession(\n",
    "    model=genai.GenerativeModel(\n",
    "        model_name='models/gemini-1.5-flash',\n",
    "        generation_config={},\n",
    "        safety_settings={},\n",
    "        tools=None,\n",
    "        system_instruction=None,\n",
    "        cached_content=None\n",
    "    ),\n",
    "    history=[]\n",
    ")\n",
    "```\n",
    "\n",
    "History can then be stored and received.\n",
    "\n",
    "```python\n",
    "response = chat.send_message(\"In one sentence, explain how a computer works to a young child.\")\n",
    "to_markdown(response.text)\n",
    "\n",
    "chat.history\n",
    "```\n",
    "\n",
    "```\n",
    "[parts {\n",
    "   text: \"In one sentence, explain how a computer works to a young child.\"\n",
    " }\n",
    " role: \"user\",\n",
    " parts {\n",
    "   text: \"A computer is like a very smart machine that can understand and follow our instructions, help us with our work, and even play games with us!\"\n",
    " }\n",
    " role: \"model\"]\n",
    "```\n",
    "\n",
    "You can iterate around the history like this:\n",
    "\n",
    "```python\n",
    "for message in chat.history:\n",
    "  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "    user: In one sentence, explain how a computer works to a young child.\n",
    "\n",
    "    model: A computer is like a very smart machine that can understand and follow our instructions, help us with our work, and even play games with us!\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c4eb9-3426-416e-ac7d-df33a274f104",
   "metadata": {},
   "source": [
    "## Counting Tokens\n",
    "\n",
    "Large language models have a context window, and the context length is often measured in terms of the number of tokens.\n",
    "\n",
    "```python\n",
    "model.count_tokens(\"What is the meaning of life?\")\n",
    "```\n",
    "\n",
    "```\n",
    "> total_tokens: 7\n",
    "```\n",
    "\n",
    "A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa0d33-8d4c-4af5-b8fd-1fae3c063219",
   "metadata": {},
   "source": [
    "## Using Embeddings\n",
    "\n",
    "Embedding is a way of representing text as a list of floats in a vector to compare and contrast embeddings. Texts that have similar subject matter or sentiment should have similar embeddings when comparing using e.g. cosine similarity.\n",
    "\n",
    "```python\n",
    "result = genai.embed_content(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    content=[\n",
    "      'What is the meaning of life?',\n",
    "      'How much wood would a woodchuck chuck?',\n",
    "      'How does the brain work?'],\n",
    "    task_type=\"retrieval_document\",\n",
    "    title=\"Embedding of list of strings\")\n",
    "\n",
    "# A list of inputs > A list of vectors output\n",
    "for v in result['embedding']:\n",
    "  print(str(v)[:50], '... TRIMMED ...')\n",
    "```\n",
    "\n",
    "```\n",
    "> [0.0040260437, 0.004124458, -0.014209415, -0.00183 ... TRIMMED ...\n",
    "```\n",
    "\n",
    "Depending on what the text is being used for, you can set different task types:\n",
    "\n",
    "Task Type | Description\n",
    "---       | ---\n",
    "RETRIEVAL_QUERY\t| Specifies the given text is a query in a search/retrieval setting.\n",
    "RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting. Using this task type requires a `title`.\n",
    "SEMANTIC_SIMILARITY\t| Specifies the given text will be used for Semantic Textual Similarity (STS).\n",
    "CLASSIFICATION\t| Specifies that the embeddings will be used for classification.\n",
    "CLUSTERING\t| Specifies that the embeddings will be used for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3835e3-70f7-4da6-9125-7290b9a86fe9",
   "metadata": {},
   "source": [
    "## Safety Settings\n",
    "\n",
    "You can set safety settings to block potentially risky prompts. The following safety filters are available:\n",
    "\n",
    "* Harassment\n",
    "* Hate speed\n",
    "* Sexually explicit\n",
    "* Dangerous\n",
    "\n",
    "Each of these categories has `HIGH` / `MEDIUM` / `LOW` / `NEGLIBIBLE` settings, and the API can be set to block each category at each of these settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f87d43-9aa4-45df-8314-428c28f19d9f",
   "metadata": {},
   "source": [
    "## Customizable Paramters\n",
    "\n",
    "\n",
    "Parameter | Description\n",
    "---       | ---\n",
    "Top p (probability) | The randomness or focus of the generated text. It specifies the probability distribution from which the next word is chosen during generation. Higher Top p (closer to 1): The model will choose the next word based on a more uniform probability distribution, leading to more creative and surprising but potentially less relevant outputs. Lower Top p (closer to 0): The model will prioritize the most likely continuations based on the current context, resulting in more predictable and relevant but potentially less creative outputs.\n",
    "Top k (number) | Limits the number of possible continuations considered by the model when generating the next word. It acts as a filter, reducing the search space for the most likely next word. Higher Top k: The model considers a wider range of possibilities, potentially leading to more diverse and interesting outputs. Lower Top k: The model focuses on a smaller set of highly likely continuations, resulting in more consistent and focused outputs.\n",
    "Temperature | Similar to Top p, controls the randomness of the generated text. However, it works by scaling the logits (log probabilities) of the candidate words before selecting the next one. Higher Temperature (greater than 1): Increases the randomness, making the model more likely to choose less probable but potentially more creative continuations. Lower Temperature (between 0 and 1): Decreases the randomness, favoring the most likely continuations and leading to more predictable outputs. Temperature of 1: Essentially acts like the original probability distribution.\n",
    "Stop Sequence (string) | This parameter specifies a string or sequence of characters that signals the end of the text generation. Once the model encounters this sequence, it will stop generating further text. This is useful for controlling the length and focus of the generated content. For example, you might set the stop sequence to a specific punctuation mark (\".\", \"?\", \"!\") to indicate the end of a sentence or paragraph.\n",
    "Max Output Length (number) | This parameter sets a hard limit on the maximum number of tokens (words or subwords) the model can generate. This helps prevent the generation of overly long or rambling outputs. It's useful when you need the generated text to be concise or fit within a specific word count.\n",
    "Number of Response Candidates (number) | This parameter (potentially specific to certain use cases) determines how many candidate continuations the model generates for each step during the generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cbc3e3-8491-4eb9-99bd-987bb29bef63",
   "metadata": {},
   "source": [
    "## System Instructions\n",
    "\n",
    "When you initialize an AI model, you can give it instructions on how to respond, such as setting a persona (\"you are a rocket scientist\") or telling it what kind of voice to use (\"talk like a pirate\"). \n",
    "\n",
    "You do this by setting the system instructions when you initialize the model.\n",
    "\n",
    "Example system instruction use cases:\n",
    "\n",
    "* Define a persona or role\n",
    "* Define output format\n",
    "* Define goals or rules\n",
    "* Provide additional context (e.g. a knowledge date cutoff)\n",
    "\n",
    "You set the instructions when you initialize the model, and then those instructions persist through all interactions with the model. The instructions persist across multiple user and model turns.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "instruction = (\n",
    "    \"You are a coding expert that specializes in front end interfaces. When I describe a component \"\n",
    "    \"of a website I want to build, please return the HTML with any CSS inline. Do not give an \"\n",
    "    \"explanation for this code.\"\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    \"models/gemini-1.5-flash\", system_instruction=instruction\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    \"A flexbox with a large text logo aligned left and a list of links aligned right.\"\n",
    ")\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Render the HTML\n",
    "HTML(response.text.strip().removeprefix(\"```html\").removesuffix(\"```\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a711a1-dce0-43fd-bedc-18fe980fa2a8",
   "metadata": {},
   "source": [
    "## Text Embeddings\n",
    "\n",
    "Text embeddings convert text into coordinates (vectors) that can be plotted in n-dimensional space. This allows text to be treated as relational data, which we can train models on.\n",
    "\n",
    "Embeddings capture semantic meaning and context, so similar sentences should have embeddings that are close to each other.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "#### Information Retrieval\n",
    "\n",
    "Use embeddings to retrieve semantically similar text given some input text. E.g. semantic search system, answering questions, or summarization.\n",
    "\n",
    "#### Classification\n",
    "\n",
    "Train a model to classify documents into categories. For example, classify user comments as negative or positive or classify forum posts into categories.\n",
    "\n",
    "#### Clustering\n",
    "\n",
    "Train a model to cluster text together. For example, cluster forum posts in a mailing list.\n",
    "\n",
    "#### Document Search\n",
    "\n",
    "Create document embeddings and then send queries to find the text that contains the most relevant answer.\n",
    "\n",
    "### Vector database\n",
    "\n",
    "You can store generated embeddings in a vector DB to improve accuracy and efficiency (AlloyDB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac18cb-5e7d-4893-bd79-9f86d40b2baa",
   "metadata": {},
   "source": [
    "## Context Caching\n",
    "\n",
    "At times, you might want to send the same input tokens over and over again for the model. E.g.\n",
    "\n",
    "* Chatbots with extensive system instructions\n",
    "* Repetitive analysis of large video files\n",
    "* Recurring queries against large document sets\n",
    "* Frequency code repo analysis\n",
    "\n",
    "To save money, instead of sending these over and over, you can cache them. Cost is based on size and cache TTL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726f0d9-08cd-4312-a0f6-50f889c2b2d3",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "LLMs break up input and produce output using tokens. Tokens could be a single character, or a whole word. A large word might be broken into several tokens.\n",
    "\n",
    "In Gemini, a token is about 4 characters.\n",
    "\n",
    "The price of a request is controlled the number of input and output tokens.\n",
    "\n",
    "### Context Window\n",
    "\n",
    "The amount of input and output the model can provide is known as the `context window`.\n",
    "\n",
    "### Multi-modal Tokens\n",
    "\n",
    "* __Images__: Images are internally a fixed size, so contain the same number of tokens\n",
    "* __Audio/Video__: Audio/video are converted to tokens at a fixed rate of tokens per minute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5796075-61b3-48ea-b5a9-f2c8cffb9960",
   "metadata": {},
   "source": [
    "## Prompting\n",
    "\n",
    "Prompts can contain:\n",
    "\n",
    "* Input (req)\n",
    "* Context (opt)\n",
    "* Examples (opt)\n",
    "\n",
    "Input can be further broken down into:\n",
    "\n",
    "* Question\n",
    "* Task (\"Give me a list of...\")\n",
    "* Entity (\"Classify / summarize...\")\n",
    "* Completion (\"Some strategies to deal with writer's block include...\")\n",
    "\n",
    "### Prompting With Media\n",
    "\n",
    "You can point the API directly at small local files, or upload files to the API for free.\n",
    "\n",
    "You can upload image, audio, video, and plain text files (including Python, CSV, JSON, etc).\n",
    "\n",
    "### Prompt Design Strategies\n",
    "\n",
    "* Make instructions clear and specific\n",
    "* Specify constraints (e.g. summarize in __two sentences__)\n",
    "* Define response format (bulleted, table, etc)\n",
    "* Provide few-shot examples (a few examples = few-shot, no examples = zero-shot)\n",
    "* If the model requires a concise response, you can give examples showing it to prefer a precise response. E.g.\n",
    "\n",
    "```\n",
    "Question: Why is sky blue?\n",
    "Explanation1: The sky appears blue because of Rayleigh scattering, which causes shorter blue\n",
    "wavelengths of light to be scattered more easily than longer red wavelengths, making the sky look\n",
    "blue.\n",
    "Explanation2: Due to Rayleigh scattering effect.\n",
    "Answer: Explanation2\n",
    "```\n",
    "\n",
    "* Find the optimal number of examples. providing too many examples may cause the model to overfit.\n",
    "* Add contextual information. For example, if prompting to troubleshoot a router, you could include context information from the router's manual and include, \"respond with only the text provided\"\n",
    "* Add prefixes\n",
    "  * Input prefixes: E.g., English: , French:...\n",
    "  * Output prefixes: E.g. JSON: signal answer should be in JSON, \"The Answer is: ...\"\n",
    "  * Example prefix\n",
    "* Let the model complete partial input\n",
    "  * LLMs work like advanced autocomplete. Given partial input, it can complete it. Giving examples or context, the model will take those into account.\n",
    "  \n",
    "_prompt:_\n",
    "```\n",
    "For the given order, return a JSON object that has the fields cheeseburger, hamburger, fries, or\n",
    "drink, with the value being the quantity.\n",
    "\n",
    "Order: A burger and a drink.\n",
    "```\n",
    "\n",
    "_response:_\n",
    "```\n",
    "{\n",
    "\"cheeseburger\": 0,\n",
    "\"hamburger\": 1,\n",
    "\"fries\": 0,\n",
    "\"drink\": 1\n",
    "}\n",
    "```\n",
    "\n",
    "Now giving it an example would cause it to exclude cheeseburger and fries.\n",
    "\n",
    "* Prompt the model to format its response\n",
    "\n",
    "_prompt:_\n",
    "```\n",
    "Create an outline for an essay about hummingbirds.\n",
    "I. Introduction\n",
    "*\n",
    "```\n",
    "\n",
    "_response:_\n",
    "```\n",
    "I. Introduction\n",
    "* Capture the reader's attention with an interesting anecdote or fact about hummingbirds.\n",
    "* Provide a brief background on hummingbirds, including their unique characteristics.\n",
    "...\n",
    "```\n",
    "\n",
    "* Experiment with parameters (see earlier section)\n",
    "\n",
    "### Breaking Down Prompts\n",
    "\n",
    "For complex prompts, you can try to break them down.\n",
    "\n",
    "#### Break down instructions\n",
    "\n",
    "Instead of having many instructions in one prompt, create one prompt per instructions, and choose which one to process based on user input.\n",
    "\n",
    "#### Chain prompts\n",
    "\n",
    "For complex tasks with sequential steps, make each step a prompt and chain them together, where the output of one because the input of the next.\n",
    "\n",
    "### Prompt Iteration\n",
    "\n",
    "It can be necessary to iterate on prompts to find the right one.\n",
    "\n",
    "* Use different phrasing\n",
    "* Switch to an analogous task (e.g., instead of \"which category...\" try \"multiple choice problem:...\"\n",
    "* Change the order of prompt content (e.g. instead of examples, context, input -- try: input, examples, context)\n",
    "\n",
    "### File Prompting\n",
    "\n",
    "Example of using images or other multimodal input:\n",
    "\n",
    "* Write a blog post based on this image\n",
    "* Get the schedule times in JSON from an image of a train platform signboard\n",
    "* Solve math problem\n",
    "* Put the table into markdown format\n",
    "* Work out the ingredients in this dish\n",
    "* Get information from product packaging, like rating or number of items in the box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3f4ea-2b49-4a47-a707-3e17e4cce6bd",
   "metadata": {},
   "source": [
    "## Semantic Retrieval (RAG)\n",
    "\n",
    "RAG can be used to augment prompts sent to the LLM with data retrived through an IR (information retrieval) system.\n",
    "\n",
    "The knowledge base can be your own corpora of docs, a DB, or APIs.\n",
    "\n",
    "We can improve LLM's responses by augmenting it with the Semantic Retriever and AQA (Attributed Question and Answering) Gemini APIs.\n",
    "\n",
    "The Semantic Retriever API lets you define up to 5 custom text corpora per project.\n",
    "\n",
    "```python\n",
    "example_corpus = glm.Corpus(display_name=\"Google for Developers Blog\")\n",
    "create_corpus_request = glm.CreateCorpusRequest(corpus=example_corpus)\n",
    "\n",
    "# Make the request\n",
    "create_corpus_response = retriever_service_client.create_corpus(create_corpus_request)\n",
    "\n",
    "# Set the `corpus_resource_name` for subsequent sections.\n",
    "corpus_resource_name = create_corpus_response.name\n",
    "print(create_corpus_response)\n",
    "```\n",
    "\n",
    "```\n",
    "name: \"corpora/google-for-developers-blog-slfs22wtfhj8\"\n",
    "display_name: \"Google for Developers Blog\"\n",
    "create_time {\n",
    "  seconds: 1721076123\n",
    "  nanos: 201645000\n",
    "}\n",
    "update_time {\n",
    "  seconds: 1721076123\n",
    "  nanos: 201645000\n",
    "}\n",
    "```\n",
    "\n",
    "* Then we add `Document`s to a corpus. Documents can also have custom metadata, such as URLs.\n",
    "\n",
    "```python\n",
    "# Create a document with a custom display name.\n",
    "example_document = glm.Document(display_name=\"Introducing Project IDX, An Experiment to Improve Full-stack, Multiplatform App Development\")\n",
    "\n",
    "# Add metadata.\n",
    "# Metadata also supports numeric values not specified here\n",
    "document_metadata = [\n",
    "    glm.CustomMetadata(key=\"url\", string_value=\"https://developers.googleblog.com/2023/08/introducing-project-idx-experiment-to-improve-full-stack-multiplatform-app-development.html\")]\n",
    "example_document.custom_metadata.extend(document_metadata)\n",
    "\n",
    "# Make the request\n",
    "# corpus_resource_name is a variable set in the \"Create a corpus\" section.\n",
    "create_document_request = glm.CreateDocumentRequest(parent=corpus_resource_name, document=example_document)\n",
    "create_document_response = retriever_service_client.create_document(create_document_request)\n",
    "\n",
    "# Set the `document_resource_name` for subsequent sections.\n",
    "document_resource_name = create_document_response.name\n",
    "print(create_document_response)\n",
    "```\n",
    "\n",
    "### Chunking\n",
    "\n",
    "To improve the relevance of content returned by the vector DB during semantic retrieval, documents can be broken down into __chunks__ while ingesting.\n",
    "\n",
    "A `Chunk` is a subpart of a `Document` that is treated as an independent unit for the purpose of vector representation and store. It can have a max of 2043 tokens.\n",
    "\n",
    "Google has it's own `HtmlChunker`, but others include `LangChain` and `LlamaIndex`.\n",
    "\n",
    "### Quering\n",
    "\n",
    "Finally, we can query the corpus. We can also set metadata filters to restrict the query to certain chunks, for example, filtering to date ranges or categories.\n",
    "\n",
    "```python\n",
    "user_query = \"What is the purpose of Project IDX?\"\n",
    "results_count = 5\n",
    "\n",
    "# Add metadata filters for both chunk and document.\n",
    "chunk_metadata_filter = glm.MetadataFilter(key='chunk.custom_metadata.tags',\n",
    "                                           conditions=[glm.Condition(\n",
    "                                              string_value='Google For Developers',\n",
    "                                              operation=glm.Condition.Operator.INCLUDES)])\n",
    "\n",
    "# Make the request\n",
    "# corpus_resource_name is a variable set in the \"Create a corpus\" section.\n",
    "request = glm.QueryCorpusRequest(name=corpus_resource_name,\n",
    "                                 query=user_query,\n",
    "                                 results_count=results_count,\n",
    "                                 metadata_filters=[chunk_metadata_filter])\n",
    "query_corpus_response = retriever_service_client.query_corpus(request)\n",
    "print(query_corpus_response)\n",
    "```\n",
    "\n",
    "### Attributed Question-Answering\n",
    "\n",
    "Use `GenerateAnswer` to perform Attributed Question-Answering on your document, corpus, or set of passes.\n",
    "\n",
    "This provides several advantages over an untuned LLM:\n",
    "\n",
    "* The underlying model has been trained to return only answers that are grounded on the supplied context\n",
    "* It identifies attributions, enabling the user to verify the answer\n",
    "* It estimates `answerable_probability`\n",
    "\n",
    "AQA is specialized for question-answering. For other use cases, such as summarization, etc., call the general model via `GenerateContent`.\n",
    "\n",
    "```python\n",
    "user_query = \"What is the purpose of Project IDX?\"\n",
    "answer_style = \"ABSTRACTIVE\" # Or VERBOSE, EXTRACTIVE\n",
    "MODEL_NAME = \"models/aqa\"\n",
    "\n",
    "# Make the request\n",
    "# corpus_resource_name is a variable set in the \"Create a corpus\" section.\n",
    "content = glm.Content(parts=[glm.Part(text=user_query)])\n",
    "retriever_config = glm.SemanticRetrieverConfig(source=corpus_resource_name, query=content)\n",
    "req = glm.GenerateAnswerRequest(model=MODEL_NAME,\n",
    "                                contents=[content],\n",
    "                                semantic_retriever=retriever_config,\n",
    "                                answer_style=answer_style)\n",
    "aqa_response = generative_service_client.generate_answer(req)\n",
    "print(aqa_response)\n",
    "```\n",
    "\n",
    "### Inline Passages\n",
    "\n",
    "Alternatively, you can bypass the Semantic Retriever API by using `inline_passages`.\n",
    "\n",
    "```python\n",
    "user_query = \"What is AQA from Google?\"\n",
    "user_query_content = glm.Content(parts=[glm.Part(text=user_query)])\n",
    "answer_style = \"VERBOSE\" # or ABSTRACTIVE, EXTRACTIVE\n",
    "MODEL_NAME = \"models/aqa\"\n",
    "\n",
    "# Create the grounding inline passages\n",
    "grounding_passages = glm.GroundingPassages()\n",
    "passage_a = glm.Content(parts=[glm.Part(text=\"Attributed Question and Answering (AQA) refers to answering questions grounded to a given corpus and providing citation\")])\n",
    "grounding_passages.passages.append(glm.GroundingPassage(content=passage_a, id=\"001\"))\n",
    "passage_b = glm.Content(parts=[glm.Part(text=\"An LLM is not designed to generate content grounded in a set of passages. Although instructing an LLM to answer questions only based on a set of passages reduces hallucination, hallucination still often occurs when LLMs generate responses unsupported by facts provided by passages\")])\n",
    "grounding_passages.passages.append(glm.GroundingPassage(content=passage_b, id=\"002\"))\n",
    "passage_c = glm.Content(parts=[glm.Part(text=\"Hallucination is one of the biggest problems in Large Language Models (LLM) development. Large Language Models (LLMs) could produce responses that are fictitious and incorrect, which significantly impacts the usefulness and trustworthiness of applications built with language models.\")])\n",
    "grounding_passages.passages.append(glm.GroundingPassage(content=passage_c, id=\"003\"))\n",
    "\n",
    "# Create the request\n",
    "req = glm.GenerateAnswerRequest(model=MODEL_NAME,\n",
    "                                contents=[user_query_content],\n",
    "                                inline_passages=grounding_passages,\n",
    "                                answer_style=answer_style)\n",
    "aqa_response = generative_service_client.generate_answer(req)\n",
    "print(aqa_response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a2437-0e77-46f4-9dd2-490ca960fa65",
   "metadata": {},
   "source": [
    "## Generative vs. Deterministic\n",
    "\n",
    "Q. Are generative models deterministic or random?\n",
    "\n",
    "A. Both.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "When you prompt the model, the text response is generated in two stages. The first stage, the model processes the input prompt and produces a __probability distribution__ over possible tokens that are likely to come next.\n",
    "\n",
    "\n",
    "E.g., if you enter, \"the dog jumped over the...\"\n",
    "\n",
    "`[(\"fence\", 0.77), (\"ledge\", 0.12), (\"blanket\", 0.03), ...]`\n",
    "\n",
    "This process is determinisitic, and the model will produce the same distribution every time.\n",
    "\n",
    "In the second stage, the generative model converts these distribution to actual text responses through one of several decoding strategies.\n",
    "\n",
    "The parameters, such as temperature, can control which token comes next, and help factor into the randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af03c63-fbf8-4416-83b4-42caf9e3c159",
   "metadata": {},
   "source": [
    "## More Use Cases\n",
    "\n",
    "* Vision: Get bounding boxes\n",
    "* Vision: Transcribe video and get visual descriptions\n",
    "* Audio: Get a transcript\n",
    "* Text: Get structured JSON from unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a636913b-13ab-4669-bba3-4004d7ccfad7",
   "metadata": {},
   "source": [
    "## Code Execution\n",
    "\n",
    "Code Execution is tool that can be made available to the model.\n",
    "\n",
    "```python\n",
    "model = genai.GenerativeModel(\n",
    "    model_name='gemini-1.5-pro',\n",
    "    tools='code_execution')\n",
    "```\n",
    "\n",
    "The model will then decide when to use it. For example\n",
    "\n",
    "```python\n",
    "response = model.generate_content((\n",
    "    'What is the sum of the first 50 prime numbers? '\n",
    "    'Generate and run code for the calculation, and make sure you get all 50.'))\n",
    "\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "```python\n",
    "```python\n",
    "def is_prime(n):\n",
    "  \"\"\"Checks if a number is prime.\"\"\"\n",
    "  if n <= 1:\n",
    "    return False\n",
    "  for i in range(2, int(n**0.5) + 1):\n",
    "    if n % i == 0:\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "def sum_of_primes(n):\n",
    "  \"\"\"Calculates the sum of the first n prime numbers.\"\"\"\n",
    "  primes = []\n",
    "  i = 2\n",
    "  while len(primes) < n:\n",
    "    if is_prime(i):\n",
    "      primes.append(i)\n",
    "    i += 1\n",
    "  return sum(primes)\n",
    "\n",
    "# Calculate the sum of the first 50 prime numbers\n",
    "sum_of_first_50_primes = sum_of_primes(50)\n",
    "\n",
    "print(f\"The sum of the first 50 prime numbers is: {sum_of_first_50_primes}\")\n",
    "\n",
    "```\n",
    "\n",
    "Alternatively, you can enable code execution in the prompt:\n",
    "\n",
    "```python\n",
    "response = model.generate_content(\n",
    "    ('What is the sum of the first 50 prime numbers? '\n",
    "    'Generate and run code for the calculation, and make sure you get all 50.'),\n",
    "    tools='code_execution')\n",
    "```\n",
    "\n",
    "The model may look at any media files you pass in to the prompt, but it won't use them as part of the code.\n",
    "\n",
    "Code execution and function calling are similar features:\n",
    "\n",
    "* Code execution lets the model run code in the API backend in a fixed, isolated environment.\n",
    "\n",
    "* Function calling lets you run the functions that the model requests, in whatever environment you want.\n",
    "In general you should prefer to use code execution if it can handle your use case. Code execution is simpler to use (you just enable it) and resolves in a single GenerateContent request (thus incurring a single charge).\n",
    "\n",
    "Function calling takes an additional GenerateContent request to send back the output from each function call (thus incurring multiple charges).\n",
    "\n",
    "For most cases, you should use function calling if you have your own functions that you want to run locally, and you should use code execution if you'd like the API to write and run Python code for you and return the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e356f016-0ac9-49c1-8ed2-7d87792ccb9b",
   "metadata": {},
   "source": [
    "## Function Calling\n",
    "\n",
    "Custom functions can be provided to Gemini models. \n",
    "\n",
    "The models do not directly invoke these functions, but generate structured data output that specifies the name and suggested args.\n",
    "\n",
    "Function calling lets you interact with real-time information and services, such as DBs, CRMs, document repos and so on.\n",
    "\n",
    "You use the Function Calling feature by adding structured query data describing programing interfaces, called function declarations, to a model prompt. \n",
    "\n",
    "The function declarations provide the name of the API function, explain its purpose, any parameters it supports, and descriptions of those parameters.\n",
    "\n",
    "After you pass a list of function declarations in a query to the model, it analyzes function declarations and the rest of the query to determine how to use the declared API in response to the request.\n",
    "\n",
    "The model then returns an object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.\n",
    "\n",
    "You can then take the recommended function call parameters, call the actual API, get a response, and provide that response to the user or take further action. \n",
    "\n",
    "It could also be used to extract structued data from text.\n",
    "\n",
    "Note: This whole section is a bit confusing, but basically what I think it does is allows you to specify some functions, and then the API will return suggested calls to those functions when it think that is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399c0792-a152-4b19-88c1-cbe72bac9038",
   "metadata": {},
   "source": [
    "## JSON Mode\n",
    "\n",
    "You can use JSON mode to...\n",
    "\n",
    "* Build a DB of companies by pulling company information from newspaper articles\n",
    "* Pull standardized info from resumes\n",
    "* Extract ingredients from recipes an ddisplay a link to the grocery story website for each ingredient\n",
    "\n",
    "You can force Gemini Pro to always respond with an expected structure by passing a JSON schema into `response_schema`.\n",
    "\n",
    "For this to work, you define a class which represents the schema you want to return.\n",
    "\n",
    "```python\n",
    "class Recipe(typing.TypedDict):\n",
    "  recipe_name: str\n",
    "\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"])\n",
    "\n",
    "# Using `response_schema` requires one of the Gemini 1.5 Pro models\n",
    "model = genai.GenerativeModel('gemini-1.5-pro',\n",
    "                              # Set the `response_mime_type` to output JSON\n",
    "                              # Pass the schema object to the `response_schema` field\n",
    "                              generation_config={\"response_mime_type\": \"application/json\",\n",
    "                                                 \"response_schema\": list[Recipe]})\n",
    "\n",
    "prompt = \"List 5 popular cookie recipes\"\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "Note, the model sometimes be able to respond with JSON without specifying a schema. E.g.:\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "  List 5 popular cookie recipes.\n",
    "  Using this JSON schema:\n",
    "    Recipe = {\"recipe_name\": str}\n",
    "  Return a `list[Recipe]`\n",
    "  \"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63758b24-4b48-4e00-b8c0-3e382677e43c",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "\n",
    "If prompts don't produce the results you want, __fine tuning__ can improve performance on a specific task, or help the model adhere to specific output requirements when you have a set of examples that show the output you want.\n",
    "\n",
    "Fine tuning works by providing the model with a training dataset that contains many examples of the task. \n",
    "\n",
    "Training data should be structured as examples with prompt inputs and expected response outputs.\n",
    "\n",
    "When you run a tuning job, the model learns additional parameters that help it encode the necessary information to perform the wanted task or learn the wanted behavior.\n",
    "\n",
    "The output of the tuning job is a new model, which is effectively a combination of the newly learned parameters, and the original model.\n",
    "\n",
    "You should target between 100-500 examples, although as little as 20 may work. Examples should be structured like real data. E.g., have `text_input` and `output` parameters.\n",
    "\n",
    "Technically, this basically works like tuning any ML model: you set up the `epoch`s, `batch size`, `learning rate` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01edb18-10d4-4592-9447-b635d2d1eadd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
