{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04269a7a-f478-4583-bf08-deb13df1d0bf",
   "metadata": {},
   "source": [
    "# Google Gemini Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e8583-169c-4ced-bef5-f3eda750db61",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "* __gemini-pro__: Optimized for high intelligence tasks, the most powerful Gemini model\n",
    "* __gemini-flash__: Optimized for multi-modal use-cases, where speed and cost are important\n",
    "* __text-embedding__: Generates text embedding.\n",
    "* __aqa__: Perform Attributed Question-Answering (AQA)â€“related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability.\n",
    "\n",
    "[Ref](https://ai.google.dev/gemini-api/docs/models/gemini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de631d5b-06c8-4f21-98b5-6da129de3713",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "\n",
    "Use the pro model:\n",
    "\n",
    "```python\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# generate_content handle various use cases, including multimodal input\n",
    "response = model.generate_content(\"What is the meaning of life?\")\n",
    "\n",
    "# Responses re given in response.text\n",
    "# You can use a method to convert the output to markdown\n",
    "to_markdown(response.text)\n",
    "\n",
    "# You can use response.prompt_feedback to understand why there was no response (e.g. there may be safety concerns)\n",
    "response.prompt_feedback\n",
    "\n",
    "# You can view multiple possible responses with response.candidates\n",
    "response.candidates\n",
    "\n",
    "# Responses can also be streamed, instead of waiting for the whole thing to be generating at once.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0830da-ac65-4d4a-a86a-34c8c203e1f1",
   "metadata": {},
   "source": [
    "## Generating Text From Images and Text Inputs\n",
    "\n",
    "```python\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "img = PIL.Image.open('image.jpg')\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "response = model.generate_content(img)\n",
    "\n",
    "to_markdown(response.text)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "> This image shows two glass containers filled with prepared food...\n",
    "\n",
    "You can also pass in a list of strings and images:\n",
    "\n",
    "```python\n",
    "response = model.generate_content([\"Write a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.\", img], stream=True)\n",
    "\n",
    "response.resolve()\n",
    "\n",
    "to_markdown(response.text)\n",
    "```\n",
    "\n",
    "> Meal prepping is a great way to save time and money, and it can also help you to eat healthier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59e1e1-61f0-4709-b270-6d12c310595f",
   "metadata": {},
   "source": [
    "## Chat Conversations\n",
    "\n",
    "You can use the `ChatSession` class to manage conversation state.\n",
    "\n",
    "```python\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "chat = model.start_chat(history=[])\n",
    "chat\n",
    "```\n",
    "\n",
    "```\n",
    "ChatSession(\n",
    "    model=genai.GenerativeModel(\n",
    "        model_name='models/gemini-1.5-flash',\n",
    "        generation_config={},\n",
    "        safety_settings={},\n",
    "        tools=None,\n",
    "        system_instruction=None,\n",
    "        cached_content=None\n",
    "    ),\n",
    "    history=[]\n",
    ")\n",
    "```\n",
    "\n",
    "History can then be stored and received.\n",
    "\n",
    "```python\n",
    "response = chat.send_message(\"In one sentence, explain how a computer works to a young child.\")\n",
    "to_markdown(response.text)\n",
    "\n",
    "chat.history\n",
    "```\n",
    "\n",
    "```\n",
    "[parts {\n",
    "   text: \"In one sentence, explain how a computer works to a young child.\"\n",
    " }\n",
    " role: \"user\",\n",
    " parts {\n",
    "   text: \"A computer is like a very smart machine that can understand and follow our instructions, help us with our work, and even play games with us!\"\n",
    " }\n",
    " role: \"model\"]\n",
    "```\n",
    "\n",
    "You can iterate around the history like this:\n",
    "\n",
    "```python\n",
    "for message in chat.history:\n",
    "  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "    user: In one sentence, explain how a computer works to a young child.\n",
    "\n",
    "    model: A computer is like a very smart machine that can understand and follow our instructions, help us with our work, and even play games with us!\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c4eb9-3426-416e-ac7d-df33a274f104",
   "metadata": {},
   "source": [
    "## Counting Tokens\n",
    "\n",
    "Large language models have a context window, and the context length is often measured in terms of the number of tokens.\n",
    "\n",
    "```python\n",
    "model.count_tokens(\"What is the meaning of life?\")\n",
    "```\n",
    "\n",
    "```\n",
    "> total_tokens: 7\n",
    "```\n",
    "\n",
    "A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa0d33-8d4c-4af5-b8fd-1fae3c063219",
   "metadata": {},
   "source": [
    "## Using Embeddings\n",
    "\n",
    "Embedding is a way of representing text as a list of floats in a vector to compare and contrast embeddings. Texts that have similar subject matter or sentiment should have similar embeddings when comparing using e.g. cosine similarity.\n",
    "\n",
    "```python\n",
    "result = genai.embed_content(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    content=[\n",
    "      'What is the meaning of life?',\n",
    "      'How much wood would a woodchuck chuck?',\n",
    "      'How does the brain work?'],\n",
    "    task_type=\"retrieval_document\",\n",
    "    title=\"Embedding of list of strings\")\n",
    "\n",
    "# A list of inputs > A list of vectors output\n",
    "for v in result['embedding']:\n",
    "  print(str(v)[:50], '... TRIMMED ...')\n",
    "```\n",
    "\n",
    "```\n",
    "> [0.0040260437, 0.004124458, -0.014209415, -0.00183 ... TRIMMED ...\n",
    "```\n",
    "\n",
    "Depending on what the text is being used for, you can set different task types:\n",
    "\n",
    "Task Type | Description\n",
    "---       | ---\n",
    "RETRIEVAL_QUERY\t| Specifies the given text is a query in a search/retrieval setting.\n",
    "RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting. Using this task type requires a `title`.\n",
    "SEMANTIC_SIMILARITY\t| Specifies the given text will be used for Semantic Textual Similarity (STS).\n",
    "CLASSIFICATION\t| Specifies that the embeddings will be used for classification.\n",
    "CLUSTERING\t| Specifies that the embeddings will be used for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3835e3-70f7-4da6-9125-7290b9a86fe9",
   "metadata": {},
   "source": [
    "## Safety Settings\n",
    "\n",
    "You can set a safety setting to block potentially risky prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f87d43-9aa4-45df-8314-428c28f19d9f",
   "metadata": {},
   "source": [
    "## Customizable Paramters\n",
    "\n",
    "\n",
    "Parameter | Description\n",
    "---       | ---\n",
    "Top p (probability) | The randomness or focus of the generated text. It specifies the probability distribution from which the next word is chosen during generation. Higher Top p (closer to 1): The model will choose the next word based on a more uniform probability distribution, leading to more creative and surprising but potentially less relevant outputs. Lower Top p (closer to 0): The model will prioritize the most likely continuations based on the current context, resulting in more predictable and relevant but potentially less creative outputs.\n",
    "Top k (number) | Limits the number of possible continuations considered by the model when generating the next word. It acts as a filter, reducing the search space for the most likely next word. Higher Top k: The model considers a wider range of possibilities, potentially leading to more diverse and interesting outputs. Lower Top k: The model focuses on a smaller set of highly likely continuations, resulting in more consistent and focused outputs.\n",
    "Temperature | Similar to Top p, controls the randomness of the generated text. However, it works by scaling the logits (log probabilities) of the candidate words before selecting the next one. Higher Temperature (greater than 1): Increases the randomness, making the model more likely to choose less probable but potentially more creative continuations. Lower Temperature (between 0 and 1): Decreases the randomness, favoring the most likely continuations and leading to more predictable outputs. Temperature of 1: Essentially acts like the original probability distribution.\n",
    "Stop Sequence (string) | This parameter specifies a string or sequence of characters that signals the end of the text generation. Once the model encounters this sequence, it will stop generating further text. This is useful for controlling the length and focus of the generated content. For example, you might set the stop sequence to a specific punctuation mark (\".\", \"?\", \"!\") to indicate the end of a sentence or paragraph.\n",
    "Max Output Length (number) | This parameter sets a hard limit on the maximum number of tokens (words or subwords) the model can generate. This helps prevent the generation of overly long or rambling outputs. It's useful when you need the generated text to be concise or fit within a specific word count.\n",
    "Number of Response Candidates (number) | This parameter (potentially specific to certain use cases) determines how many candidate continuations the model generates for each step during the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106c16c-e775-43fc-a830-67b2edc1ea14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
