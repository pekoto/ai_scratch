{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04269a7a-f478-4583-bf08-deb13df1d0bf",
   "metadata": {},
   "source": [
    "# Google Gemini Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e8583-169c-4ced-bef5-f3eda750db61",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "* __gemini-pro__: Optimized for high intelligence tasks, the most powerful Gemini model\n",
    "* __gemini-flash__: Optimized for multi-modal use-cases, where speed and cost are important\n",
    "* __text-embedding__: Generates text embedding.\n",
    "* __aqa__: Perform Attributed Question-Answering (AQA)â€“related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability.\n",
    "\n",
    "[Ref](https://ai.google.dev/gemini-api/docs/models/gemini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de631d5b-06c8-4f21-98b5-6da129de3713",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "\n",
    "Use the pro model:\n",
    "\n",
    "```python\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# generate_content handle various use cases, including multimodal input\n",
    "response = model.generate_content(\"What is the meaning of life?\")\n",
    "\n",
    "# Responses re given in response.text\n",
    "# You can use a method to convert the output to markdown\n",
    "to_markdown(response.text)\n",
    "\n",
    "# You can use response.prompt_feedback to understand why there was no response (e.g. there may be safety concerns)\n",
    "response.prompt_feedback\n",
    "\n",
    "# You can view multiple possible responses with response.candidates\n",
    "response.candidates\n",
    "\n",
    "# Responses can also be streamed, instead of waiting for the whole thing to be generating at once.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0830da-ac65-4d4a-a86a-34c8c203e1f1",
   "metadata": {},
   "source": [
    "## Generating Text From Images and Text Inputs\n",
    "\n",
    "```python\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "img = PIL.Image.open('image.jpg')\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "response = model.generate_content(img)\n",
    "\n",
    "to_markdown(response.text)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "> This image shows two glass containers filled with prepared food...\n",
    "\n",
    "You can also pass in a list of strings and images:\n",
    "\n",
    "```python\n",
    "response = model.generate_content([\"Write a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.\", img], stream=True)\n",
    "\n",
    "response.resolve()\n",
    "\n",
    "to_markdown(response.text)\n",
    "```\n",
    "\n",
    "> Meal prepping is a great way to save time and money, and it can also help you to eat healthier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59e1e1-61f0-4709-b270-6d12c310595f",
   "metadata": {},
   "source": [
    "## Chat Conversations\n",
    "\n",
    "You can use the `ChatSession` class to manage conversation state.\n",
    "\n",
    "```python\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "chat = model.start_chat(history=[])\n",
    "chat\n",
    "```\n",
    "\n",
    "```\n",
    "ChatSession(\n",
    "    model=genai.GenerativeModel(\n",
    "        model_name='models/gemini-1.5-flash',\n",
    "        generation_config={},\n",
    "        safety_settings={},\n",
    "        tools=None,\n",
    "        system_instruction=None,\n",
    "        cached_content=None\n",
    "    ),\n",
    "    history=[]\n",
    ")\n",
    "```\n",
    "\n",
    "History can then be stored and received.\n",
    "\n",
    "```python\n",
    "response = chat.send_message(\"In one sentence, explain how a computer works to a young child.\")\n",
    "to_markdown(response.text)\n",
    "\n",
    "chat.history\n",
    "```\n",
    "\n",
    "```\n",
    "[parts {\n",
    "   text: \"In one sentence, explain how a computer works to a young child.\"\n",
    " }\n",
    " role: \"user\",\n",
    " parts {\n",
    "   text: \"A computer is like a very smart machine that can understand and follow our instructions, help us with our work, and even play games with us!\"\n",
    " }\n",
    " role: \"model\"]\n",
    "```\n",
    "\n",
    "You can iterate around the history like this:\n",
    "\n",
    "```python\n",
    "for message in chat.history:\n",
    "  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "    user: In one sentence, explain how a computer works to a young child.\n",
    "\n",
    "    model: A computer is like a very smart machine that can understand and follow our instructions, help us with our work, and even play games with us!\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c4eb9-3426-416e-ac7d-df33a274f104",
   "metadata": {},
   "source": [
    "## Counting Tokens\n",
    "\n",
    "Large language models have a context window, and the context length is often measured in terms of the number of tokens.\n",
    "\n",
    "```python\n",
    "model.count_tokens(\"What is the meaning of life?\")\n",
    "```\n",
    "\n",
    "```\n",
    "> total_tokens: 7\n",
    "```\n",
    "\n",
    "A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa0d33-8d4c-4af5-b8fd-1fae3c063219",
   "metadata": {},
   "source": [
    "## Using Embeddings\n",
    "\n",
    "Embedding is a way of representing text as a list of floats in a vector to compare and contrast embeddings. Texts that have similar subject matter or sentiment should have similar embeddings when comparing using e.g. cosine similarity.\n",
    "\n",
    "```python\n",
    "result = genai.embed_content(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    content=[\n",
    "      'What is the meaning of life?',\n",
    "      'How much wood would a woodchuck chuck?',\n",
    "      'How does the brain work?'],\n",
    "    task_type=\"retrieval_document\",\n",
    "    title=\"Embedding of list of strings\")\n",
    "\n",
    "# A list of inputs > A list of vectors output\n",
    "for v in result['embedding']:\n",
    "  print(str(v)[:50], '... TRIMMED ...')\n",
    "```\n",
    "\n",
    "```\n",
    "> [0.0040260437, 0.004124458, -0.014209415, -0.00183 ... TRIMMED ...\n",
    "```\n",
    "\n",
    "Depending on what the text is being used for, you can set different task types:\n",
    "\n",
    "Task Type | Description\n",
    "---       | ---\n",
    "RETRIEVAL_QUERY\t| Specifies the given text is a query in a search/retrieval setting.\n",
    "RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting. Using this task type requires a `title`.\n",
    "SEMANTIC_SIMILARITY\t| Specifies the given text will be used for Semantic Textual Similarity (STS).\n",
    "CLASSIFICATION\t| Specifies that the embeddings will be used for classification.\n",
    "CLUSTERING\t| Specifies that the embeddings will be used for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3835e3-70f7-4da6-9125-7290b9a86fe9",
   "metadata": {},
   "source": [
    "## Safety Settings\n",
    "\n",
    "You can set safety settings to block potentially risky prompts. The following safety filters are available:\n",
    "\n",
    "* Harassment\n",
    "* Hate speed\n",
    "* Sexually explicit\n",
    "* Dangerous\n",
    "\n",
    "Each of these categories has `HIGH` / `MEDIUM` / `LOW` / `NEGLIBIBLE` settings, and the API can be set to block each category at each of these settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f87d43-9aa4-45df-8314-428c28f19d9f",
   "metadata": {},
   "source": [
    "## Customizable Paramters\n",
    "\n",
    "\n",
    "Parameter | Description\n",
    "---       | ---\n",
    "Top p (probability) | The randomness or focus of the generated text. It specifies the probability distribution from which the next word is chosen during generation. Higher Top p (closer to 1): The model will choose the next word based on a more uniform probability distribution, leading to more creative and surprising but potentially less relevant outputs. Lower Top p (closer to 0): The model will prioritize the most likely continuations based on the current context, resulting in more predictable and relevant but potentially less creative outputs.\n",
    "Top k (number) | Limits the number of possible continuations considered by the model when generating the next word. It acts as a filter, reducing the search space for the most likely next word. Higher Top k: The model considers a wider range of possibilities, potentially leading to more diverse and interesting outputs. Lower Top k: The model focuses on a smaller set of highly likely continuations, resulting in more consistent and focused outputs.\n",
    "Temperature | Similar to Top p, controls the randomness of the generated text. However, it works by scaling the logits (log probabilities) of the candidate words before selecting the next one. Higher Temperature (greater than 1): Increases the randomness, making the model more likely to choose less probable but potentially more creative continuations. Lower Temperature (between 0 and 1): Decreases the randomness, favoring the most likely continuations and leading to more predictable outputs. Temperature of 1: Essentially acts like the original probability distribution.\n",
    "Stop Sequence (string) | This parameter specifies a string or sequence of characters that signals the end of the text generation. Once the model encounters this sequence, it will stop generating further text. This is useful for controlling the length and focus of the generated content. For example, you might set the stop sequence to a specific punctuation mark (\".\", \"?\", \"!\") to indicate the end of a sentence or paragraph.\n",
    "Max Output Length (number) | This parameter sets a hard limit on the maximum number of tokens (words or subwords) the model can generate. This helps prevent the generation of overly long or rambling outputs. It's useful when you need the generated text to be concise or fit within a specific word count.\n",
    "Number of Response Candidates (number) | This parameter (potentially specific to certain use cases) determines how many candidate continuations the model generates for each step during the generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cbc3e3-8491-4eb9-99bd-987bb29bef63",
   "metadata": {},
   "source": [
    "## System Instructions\n",
    "\n",
    "When you initialize an AI model, you can give it instructions on how to respond, such as setting a persona (\"you are a rocket scientist\") or telling it what kind of voice to use (\"talk like a pirate\"). \n",
    "\n",
    "You do this by setting the system instructions when you initialize the model.\n",
    "\n",
    "Example system instruction use cases:\n",
    "\n",
    "* Define a persona or role\n",
    "* Define output format\n",
    "* Define goals or rules\n",
    "* Provide additional context (e.g. a knowledge date cutoff)\n",
    "\n",
    "You set the instructions when you initialize the model, and then those instructions persist through all interactions with the model. The instructions persist across multiple user and model turns.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "instruction = (\n",
    "    \"You are a coding expert that specializes in front end interfaces. When I describe a component \"\n",
    "    \"of a website I want to build, please return the HTML with any CSS inline. Do not give an \"\n",
    "    \"explanation for this code.\"\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    \"models/gemini-1.5-flash\", system_instruction=instruction\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    \"A flexbox with a large text logo aligned left and a list of links aligned right.\"\n",
    ")\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Render the HTML\n",
    "HTML(response.text.strip().removeprefix(\"```html\").removesuffix(\"```\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a711a1-dce0-43fd-bedc-18fe980fa2a8",
   "metadata": {},
   "source": [
    "## Text Embeddings\n",
    "\n",
    "Text embeddings convert text into coordinates (vectors) that can be plotted in n-dimensional space. This allows text to be treated as relational data, which we can train models on.\n",
    "\n",
    "Embeddings capture semantic meaning and context, so similar sentences should have embeddings that are close to each other.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "#### Information Retrieval\n",
    "\n",
    "Use embeddings to retrieve semantically similar text given some input text. E.g. semantic search system, answering questions, or summarization.\n",
    "\n",
    "#### Classification\n",
    "\n",
    "Train a model to classify documents into categories. For example, classify user comments as negative or positive or classify forum posts into categories.\n",
    "\n",
    "#### Clustering\n",
    "\n",
    "Train a model to cluster text together. For example, cluster forum posts in a mailing list.\n",
    "\n",
    "#### Document Search\n",
    "\n",
    "Create document embeddings and then send queries to find the text that contains the most relevant answer.\n",
    "\n",
    "### Vector database\n",
    "\n",
    "You can store generated embeddings in a vector DB to improve accuracy and efficiency (AlloyDB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac18cb-5e7d-4893-bd79-9f86d40b2baa",
   "metadata": {},
   "source": [
    "## Context Caching\n",
    "\n",
    "At times, you might want to send the same input tokens over and over again for the model. E.g.\n",
    "\n",
    "* Chatbots with extensive system instructions\n",
    "* Repetitive analysis of large video files\n",
    "* Recurring queries against large document sets\n",
    "* Frequency code repo analysis\n",
    "\n",
    "To save money, instead of sending these over and over, you can cache them. Cost is based on size and cache TTL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726f0d9-08cd-4312-a0f6-50f889c2b2d3",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "LLMs break up input and produce output using tokens. Tokens could be a single character, or a whole word. A large word might be broken into several tokens.\n",
    "\n",
    "In Gemini, a token is about 4 characters.\n",
    "\n",
    "The price of a request is controlled the number of input and output tokens.\n",
    "\n",
    "### Context Window\n",
    "\n",
    "The amount of input and output the model can provide is known as the `context window`.\n",
    "\n",
    "### Multi-modal Tokens\n",
    "\n",
    "* __Images__: Images are internally a fixed size, so contain the same number of tokens\n",
    "* __Audio/Video__: Audio/video are converted to tokens at a fixed rate of tokens per minute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5796075-61b3-48ea-b5a9-f2c8cffb9960",
   "metadata": {},
   "source": [
    "## Prompting\n",
    "\n",
    "Prompts can contain:\n",
    "\n",
    "* Input (req)\n",
    "* Context (opt)\n",
    "* Examples (opt)\n",
    "\n",
    "Input can be further broken down into:\n",
    "\n",
    "* Question\n",
    "* Task (\"Give me a list of...\")\n",
    "* Entity (\"Classify / summarize...\")\n",
    "* Completion (\"Some strategies to deal with writer's block include...\")\n",
    "\n",
    "### Prompting With Media\n",
    "\n",
    "You can point the API directly at small local files, or upload files to the API for free.\n",
    "\n",
    "You can upload image, audio, video, and plain text files (including Python, CSV, JSON, etc).\n",
    "\n",
    "### Prompt Design Strategies\n",
    "\n",
    "* Make instructions clear and specific\n",
    "* Specify constraints (e.g. summarize in __two sentences__)\n",
    "* Define response format (bulleted, table, etc)\n",
    "* Provide few-shot examples (a few examples = few-shot, no examples = zero-shot)\n",
    "* If the model requires a concise response, you can give examples showing it to prefer a precise response. E.g.\n",
    "\n",
    "```\n",
    "Question: Why is sky blue?\n",
    "Explanation1: The sky appears blue because of Rayleigh scattering, which causes shorter blue\n",
    "wavelengths of light to be scattered more easily than longer red wavelengths, making the sky look\n",
    "blue.\n",
    "Explanation2: Due to Rayleigh scattering effect.\n",
    "Answer: Explanation2\n",
    "```\n",
    "\n",
    "* Find the optimal number of examples. providing too many examples may cause the model to overfit.\n",
    "* Add contextual information. For example, if prompting to troubleshoot a router, you could include context information from the router's manual and include, \"respond with only the text provided\"\n",
    "* Add prefixes\n",
    "  * Input prefixes: E.g., English: , French:...\n",
    "  * Output prefixes: E.g. JSON: signal answer should be in JSON, \"The Answer is: ...\"\n",
    "  * Example prefix\n",
    "* Let the model complete partial input\n",
    "  * LLMs work like advanced autocomplete. Given partial input, it can complete it. Giving examples or context, the model will take those into account.\n",
    "  \n",
    "_prompt:_\n",
    "```\n",
    "For the given order, return a JSON object that has the fields cheeseburger, hamburger, fries, or\n",
    "drink, with the value being the quantity.\n",
    "\n",
    "Order: A burger and a drink.\n",
    "```\n",
    "\n",
    "_response:_\n",
    "```\n",
    "{\n",
    "\"cheeseburger\": 0,\n",
    "\"hamburger\": 1,\n",
    "\"fries\": 0,\n",
    "\"drink\": 1\n",
    "}\n",
    "```\n",
    "\n",
    "Now giving it an example would cause it to exclude cheeseburger and fries.\n",
    "\n",
    "* Prompt the model to format its response\n",
    "\n",
    "_prompt:_\n",
    "```\n",
    "Create an outline for an essay about hummingbirds.\n",
    "I. Introduction\n",
    "*\n",
    "```\n",
    "\n",
    "_response:_\n",
    "```\n",
    "I. Introduction\n",
    "* Capture the reader's attention with an interesting anecdote or fact about hummingbirds.\n",
    "* Provide a brief background on hummingbirds, including their unique characteristics.\n",
    "...\n",
    "```\n",
    "\n",
    "* Experiment with parameters (see earlier section)\n",
    "\n",
    "### Breaking Down Prompts\n",
    "\n",
    "For complex prompts, you can try to break them down.\n",
    "\n",
    "#### Break down instructions\n",
    "\n",
    "Instead of having many instructions in one prompt, create one prompt per instructions, and choose which one to process based on user input.\n",
    "\n",
    "#### Chain prompts\n",
    "\n",
    "For complex tasks with sequential steps, make each step a prompt and chain them together, where the output of one because the input of the next.\n",
    "\n",
    "### Prompt Iteration\n",
    "\n",
    "It can be necessary to iterate on prompts to find the right one.\n",
    "\n",
    "* Use different phrasing\n",
    "* Switch to an analogous task (e.g., instead of \"which category...\" try \"multiple choice problem:...\"\n",
    "* Change the order of prompt content (e.g. instead of examples, context, input -- try: input, examples, context)\n",
    "\n",
    "### File Prompting\n",
    "\n",
    "Example of using images or other multimodal input:\n",
    "\n",
    "* Write a blog post based on this image\n",
    "* Get the schedule times in JSON from an image of a train platform signboard\n",
    "* Solve math problem\n",
    "* Put the table into markdown format\n",
    "* Work out the ingredients in this dish\n",
    "* Get information from product packaging, like rating or number of items in the box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3f4ea-2b49-4a47-a707-3e17e4cce6bd",
   "metadata": {},
   "source": [
    "## Semantic Retrieval (RAG)\n",
    "\n",
    "RAG can be used to augment prompts sent to the LLM with data retrived through an IR (information retrieval) system.\n",
    "\n",
    "The knowledge base can be your own corpora of docs, a DB, or APIs.\n",
    "\n",
    "We can improve LLM's responses by augmenting it with the Semantic Retriever and AQA (Attributed Question and Answering) Gemini APIs.\n",
    "\n",
    "The Semantic Retriever API lets you define up to 5 custom text corpora per project.\n",
    "\n",
    "```python\n",
    "example_corpus = glm.Corpus(display_name=\"Google for Developers Blog\")\n",
    "create_corpus_request = glm.CreateCorpusRequest(corpus=example_corpus)\n",
    "\n",
    "# Make the request\n",
    "create_corpus_response = retriever_service_client.create_corpus(create_corpus_request)\n",
    "\n",
    "# Set the `corpus_resource_name` for subsequent sections.\n",
    "corpus_resource_name = create_corpus_response.name\n",
    "print(create_corpus_response)\n",
    "```\n",
    "\n",
    "```\n",
    "name: \"corpora/google-for-developers-blog-slfs22wtfhj8\"\n",
    "display_name: \"Google for Developers Blog\"\n",
    "create_time {\n",
    "  seconds: 1721076123\n",
    "  nanos: 201645000\n",
    "}\n",
    "update_time {\n",
    "  seconds: 1721076123\n",
    "  nanos: 201645000\n",
    "}\n",
    "```\n",
    "\n",
    "* Then we add `Document`s to a corpus. Documents can also have custom metadata, such as URLs.\n",
    "\n",
    "```python\n",
    "# Create a document with a custom display name.\n",
    "example_document = glm.Document(display_name=\"Introducing Project IDX, An Experiment to Improve Full-stack, Multiplatform App Development\")\n",
    "\n",
    "# Add metadata.\n",
    "# Metadata also supports numeric values not specified here\n",
    "document_metadata = [\n",
    "    glm.CustomMetadata(key=\"url\", string_value=\"https://developers.googleblog.com/2023/08/introducing-project-idx-experiment-to-improve-full-stack-multiplatform-app-development.html\")]\n",
    "example_document.custom_metadata.extend(document_metadata)\n",
    "\n",
    "# Make the request\n",
    "# corpus_resource_name is a variable set in the \"Create a corpus\" section.\n",
    "create_document_request = glm.CreateDocumentRequest(parent=corpus_resource_name, document=example_document)\n",
    "create_document_response = retriever_service_client.create_document(create_document_request)\n",
    "\n",
    "# Set the `document_resource_name` for subsequent sections.\n",
    "document_resource_name = create_document_response.name\n",
    "print(create_document_response)\n",
    "```\n",
    "\n",
    "### Chunking\n",
    "\n",
    "To improve the relevance of content returned by the vector DB during semantic retrieval, documents can be broken down into __chunks__ while ingesting.\n",
    "\n",
    "A `Chunk` is a subpart of a `Document` that is treated as an independent unit for the purpose of vector representation and store. It can have a max of 2043 tokens.\n",
    "\n",
    "Google has it's own `HtmlChunker`, but others include `LangChain` and `LlamaIndex`.\n",
    "\n",
    "### Quering\n",
    "\n",
    "Finally, we can query the corpus. We can also set metadata filters to restrict the query to certain chunks, for example, filtering to date ranges or categories.\n",
    "\n",
    "```python\n",
    "user_query = \"What is the purpose of Project IDX?\"\n",
    "results_count = 5\n",
    "\n",
    "# Add metadata filters for both chunk and document.\n",
    "chunk_metadata_filter = glm.MetadataFilter(key='chunk.custom_metadata.tags',\n",
    "                                           conditions=[glm.Condition(\n",
    "                                              string_value='Google For Developers',\n",
    "                                              operation=glm.Condition.Operator.INCLUDES)])\n",
    "\n",
    "# Make the request\n",
    "# corpus_resource_name is a variable set in the \"Create a corpus\" section.\n",
    "request = glm.QueryCorpusRequest(name=corpus_resource_name,\n",
    "                                 query=user_query,\n",
    "                                 results_count=results_count,\n",
    "                                 metadata_filters=[chunk_metadata_filter])\n",
    "query_corpus_response = retriever_service_client.query_corpus(request)\n",
    "print(query_corpus_response)\n",
    "```\n",
    "\n",
    "### Attributed Question-Answering\n",
    "\n",
    "Use `GenerateAnswer` to perform Attributed Question-Answering on your document, corpus, or set of passes.\n",
    "\n",
    "This provides several advantages over an untuned LLM:\n",
    "\n",
    "* The underlying model has been trained to return only answers that are grounded on the supplied context\n",
    "* It identifies attributions, enabling the user to verify the answer\n",
    "* It estimates `answerable_probability`\n",
    "\n",
    "AQA is specialized for question-answering. For other use cases, such as summarization, etc., call the general model via `GenerateContent`.\n",
    "\n",
    "```python\n",
    "user_query = \"What is the purpose of Project IDX?\"\n",
    "answer_style = \"ABSTRACTIVE\" # Or VERBOSE, EXTRACTIVE\n",
    "MODEL_NAME = \"models/aqa\"\n",
    "\n",
    "# Make the request\n",
    "# corpus_resource_name is a variable set in the \"Create a corpus\" section.\n",
    "content = glm.Content(parts=[glm.Part(text=user_query)])\n",
    "retriever_config = glm.SemanticRetrieverConfig(source=corpus_resource_name, query=content)\n",
    "req = glm.GenerateAnswerRequest(model=MODEL_NAME,\n",
    "                                contents=[content],\n",
    "                                semantic_retriever=retriever_config,\n",
    "                                answer_style=answer_style)\n",
    "aqa_response = generative_service_client.generate_answer(req)\n",
    "print(aqa_response)\n",
    "```\n",
    "\n",
    "### Inline Passages\n",
    "\n",
    "Alternatively, you can bypass the Semantic Retriever API by using `inline_passages`.\n",
    "\n",
    "```python\n",
    "user_query = \"What is AQA from Google?\"\n",
    "user_query_content = glm.Content(parts=[glm.Part(text=user_query)])\n",
    "answer_style = \"VERBOSE\" # or ABSTRACTIVE, EXTRACTIVE\n",
    "MODEL_NAME = \"models/aqa\"\n",
    "\n",
    "# Create the grounding inline passages\n",
    "grounding_passages = glm.GroundingPassages()\n",
    "passage_a = glm.Content(parts=[glm.Part(text=\"Attributed Question and Answering (AQA) refers to answering questions grounded to a given corpus and providing citation\")])\n",
    "grounding_passages.passages.append(glm.GroundingPassage(content=passage_a, id=\"001\"))\n",
    "passage_b = glm.Content(parts=[glm.Part(text=\"An LLM is not designed to generate content grounded in a set of passages. Although instructing an LLM to answer questions only based on a set of passages reduces hallucination, hallucination still often occurs when LLMs generate responses unsupported by facts provided by passages\")])\n",
    "grounding_passages.passages.append(glm.GroundingPassage(content=passage_b, id=\"002\"))\n",
    "passage_c = glm.Content(parts=[glm.Part(text=\"Hallucination is one of the biggest problems in Large Language Models (LLM) development. Large Language Models (LLMs) could produce responses that are fictitious and incorrect, which significantly impacts the usefulness and trustworthiness of applications built with language models.\")])\n",
    "grounding_passages.passages.append(glm.GroundingPassage(content=passage_c, id=\"003\"))\n",
    "\n",
    "# Create the request\n",
    "req = glm.GenerateAnswerRequest(model=MODEL_NAME,\n",
    "                                contents=[user_query_content],\n",
    "                                inline_passages=grounding_passages,\n",
    "                                answer_style=answer_style)\n",
    "aqa_response = generative_service_client.generate_answer(req)\n",
    "print(aqa_response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a2437-0e77-46f4-9dd2-490ca960fa65",
   "metadata": {},
   "source": [
    "## Generative vs. Deterministic\n",
    "\n",
    "Q. Are generative models deterministic or random?\n",
    "\n",
    "A. Both.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "When you prompt the model, the text response is generated in two stages. The first stage, the model processes the input prompt and produces a __probability distribution__ over possible tokens that are likely to come next.\n",
    "\n",
    "\n",
    "E.g., if you enter, \"the dog jumped over the...\"\n",
    "\n",
    "`[(\"fence\", 0.77), (\"ledge\", 0.12), (\"blanket\", 0.03), ...]`\n",
    "\n",
    "This process is determinisitic, and the model will produce the same distribution every time.\n",
    "\n",
    "In the second stage, the generative model converts these distribution to actual text responses through one of several decoding strategies.\n",
    "\n",
    "The parameters, such as temperature, can control which token comes next, and help factor into the randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af03c63-fbf8-4416-83b4-42caf9e3c159",
   "metadata": {},
   "source": [
    "## More Use Cases\n",
    "\n",
    "* Vision: Get bounding boxes\n",
    "* Vision: Transcribe video and get visual descriptions\n",
    "* Audio: Get a transcript\n",
    "* Text: Get structured JSON from unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a636913b-13ab-4669-bba3-4004d7ccfad7",
   "metadata": {},
   "source": [
    "## Code Execution\n",
    "\n",
    "Code Execution is tool that can be made available to the model.\n",
    "\n",
    "```python\n",
    "model = genai.GenerativeModel(\n",
    "    model_name='gemini-1.5-pro',\n",
    "    tools='code_execution')\n",
    "```\n",
    "\n",
    "The model will then decide when to use it. For example\n",
    "\n",
    "```python\n",
    "response = model.generate_content((\n",
    "    'What is the sum of the first 50 prime numbers? '\n",
    "    'Generate and run code for the calculation, and make sure you get all 50.'))\n",
    "\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "```python\n",
    "```python\n",
    "def is_prime(n):\n",
    "  \"\"\"Checks if a number is prime.\"\"\"\n",
    "  if n <= 1:\n",
    "    return False\n",
    "  for i in range(2, int(n**0.5) + 1):\n",
    "    if n % i == 0:\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "def sum_of_primes(n):\n",
    "  \"\"\"Calculates the sum of the first n prime numbers.\"\"\"\n",
    "  primes = []\n",
    "  i = 2\n",
    "  while len(primes) < n:\n",
    "    if is_prime(i):\n",
    "      primes.append(i)\n",
    "    i += 1\n",
    "  return sum(primes)\n",
    "\n",
    "# Calculate the sum of the first 50 prime numbers\n",
    "sum_of_first_50_primes = sum_of_primes(50)\n",
    "\n",
    "print(f\"The sum of the first 50 prime numbers is: {sum_of_first_50_primes}\")\n",
    "\n",
    "```\n",
    "\n",
    "Alternatively, you can enable code execution in the prompt:\n",
    "\n",
    "```python\n",
    "response = model.generate_content(\n",
    "    ('What is the sum of the first 50 prime numbers? '\n",
    "    'Generate and run code for the calculation, and make sure you get all 50.'),\n",
    "    tools='code_execution')\n",
    "```\n",
    "\n",
    "The model may look at any media files you pass in to the prompt, but it won't use them as part of the code.\n",
    "\n",
    "Code execution and function calling are similar features:\n",
    "\n",
    "* Code execution lets the model run code in the API backend in a fixed, isolated environment.\n",
    "\n",
    "* Function calling lets you run the functions that the model requests, in whatever environment you want.\n",
    "In general you should prefer to use code execution if it can handle your use case. Code execution is simpler to use (you just enable it) and resolves in a single GenerateContent request (thus incurring a single charge).\n",
    "\n",
    "* Function calling takes an additional GenerateContent request to send back the output from each function call (thus incurring multiple charges).\n",
    "\n",
    "For most cases, you should use function calling if you have your own functions that you want to run locally, and you should use code execution if you'd like the API to write and run Python code for you and return the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e356f016-0ac9-49c1-8ed2-7d87792ccb9b",
   "metadata": {},
   "source": [
    "## Function Calling\n",
    "\n",
    "- You provide function definitions in your prompt. These definitions include the function name, purpose, parameters, and parameter descriptions.\n",
    "\n",
    "- Gemini analyzes your prompt and function definitions.\n",
    "\n",
    "- The model recommends function calls (including parameters) to answer your prompt.\n",
    "\n",
    "- __You__ call the actual API using the recommended parameters and handle the response. Gemini doesn't directly call the functions.\n",
    "\n",
    "Use code execution: If your task involves calculations, data processing, or problems solvable by Python code within the API.\n",
    "Use function calling: If you need to access external APIs with real-time data or prefer to use your custom functions in your own environment.\n",
    "\n",
    "Custom functions can be provided to Gemini models. \n",
    "\n",
    "The models do not directly invoke these functions, but generate structured data output that specifies the name and suggested args.\n",
    "\n",
    "Function calling lets you interact with real-time information and services, such as DBs, CRMs, document repos and so on.\n",
    "\n",
    "You use the Function Calling feature by adding structured query data describing programing interfaces, called function declarations, to a model prompt. \n",
    "\n",
    "The function declarations provide the name of the API function, explain its purpose, any parameters it supports, and descriptions of those parameters.\n",
    "\n",
    "After you pass a list of function declarations in a query to the model, it analyzes function declarations and the rest of the query to determine how to use the declared API in response to the request.\n",
    "\n",
    "The model then returns an object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.\n",
    "\n",
    "You can then take the recommended function call parameters, call the actual API, get a response, and provide that response to the user or take further action. \n",
    "\n",
    "It could also be used to extract structued data from text.\n",
    "\n",
    "Note: This whole section is a bit confusing, but basically what I think it does is allows you to specify some functions, and then the API will return suggested calls to those functions when it think that is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399c0792-a152-4b19-88c1-cbe72bac9038",
   "metadata": {},
   "source": [
    "## JSON Mode\n",
    "\n",
    "You can use JSON mode to...\n",
    "\n",
    "* Build a DB of companies by pulling company information from newspaper articles\n",
    "* Pull standardized info from resumes\n",
    "* Extract ingredients from recipes an ddisplay a link to the grocery story website for each ingredient\n",
    "\n",
    "You can force Gemini Pro to always respond with an expected structure by passing a JSON schema into `response_schema`.\n",
    "\n",
    "For this to work, you define a class which represents the schema you want to return.\n",
    "\n",
    "```python\n",
    "class Recipe(typing.TypedDict):\n",
    "  recipe_name: str\n",
    "\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"])\n",
    "\n",
    "# Using `response_schema` requires one of the Gemini 1.5 Pro models\n",
    "model = genai.GenerativeModel('gemini-1.5-pro',\n",
    "                              # Set the `response_mime_type` to output JSON\n",
    "                              # Pass the schema object to the `response_schema` field\n",
    "                              generation_config={\"response_mime_type\": \"application/json\",\n",
    "                                                 \"response_schema\": list[Recipe]})\n",
    "\n",
    "prompt = \"List 5 popular cookie recipes\"\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "Note, the model sometimes be able to respond with JSON without specifying a schema. E.g.:\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "  List 5 popular cookie recipes.\n",
    "  Using this JSON schema:\n",
    "    Recipe = {\"recipe_name\": str}\n",
    "  Return a `list[Recipe]`\n",
    "  \"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63758b24-4b48-4e00-b8c0-3e382677e43c",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "\n",
    "If prompts don't produce the results you want, __fine tuning__ can improve performance on a specific task, or help the model adhere to specific output requirements when you have a set of examples that show the output you want.\n",
    "\n",
    "Fine tuning works by providing the model with a training dataset that contains many examples of the task. \n",
    "\n",
    "Training data should be structured as examples with prompt inputs and expected response outputs.\n",
    "\n",
    "When you run a tuning job, the model learns additional parameters that help it encode the necessary information to perform the wanted task or learn the wanted behavior.\n",
    "\n",
    "The output of the tuning job is a new model, which is effectively a combination of the newly learned parameters, and the original model.\n",
    "\n",
    "You should target between 100-500 examples, although as little as 20 may work. Examples should be structured like real data. E.g., have `text_input` and `output` parameters.\n",
    "\n",
    "Technically, this basically works like tuning any ML model: you set up the `epoch`s, `batch size`, `learning rate` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac73130-d291-456a-a3c4-a7f5033c01fa",
   "metadata": {},
   "source": [
    "## Google AI Studio\n",
    "\n",
    "A web tool that lets you craft, test, and save prompts for different models.\n",
    "\n",
    "Three core tools:\n",
    "\n",
    "* Freeform prompts (<= 30k tokens)\n",
    "* Structed prompts (can provide prompt example table of input and output. E.g. input > word, output > the opposite, or classification)\n",
    "\n",
    "Then you can export the prompts as code.\n",
    "\n",
    "AI Studio also supports fine tuning, and can fine tune using Sheets or CSV data (or add examples).\n",
    "\n",
    "A nice feature is that you can add the Gemini generated responses to the examples, reinforcing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08658147-e13a-4871-a49c-5b664699911f",
   "metadata": {},
   "source": [
    "## ReAct Prompting\n",
    "\n",
    "ReAct prompting = Reasoning and Acting.\n",
    "\n",
    "A ReAct prompt is a new approach to prompting large language models (LLMs) that combines reasoning and acting. It allows the LLM to perform dynamic reasoning to create, maintain, and adjust plans for acting while also enabling interaction to external environments (e.g., Wikipedia) to incorporate additional information into the reasoning.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "model_instructions = \"\"\"Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, Observation is understanding relevant information from an Action's output and Action can be of three types:\n",
    "(1) entity, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search and you can try to search the information from those topics.\n",
    "(2) keyword, which returns the next sentence containing keyword in the current context. This only does exact matches, so keep your searches short.\n",
    "(3) answer, which returns the answer and finishes the task.\n",
    "\n",
    "examples = \"\"\"\n",
    "Here are some examples.\n",
    "\n",
    "Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "Colorado orogeny\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "eastern sector\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc5b5d-5dcc-4a2e-b007-d53420a06c49",
   "metadata": {},
   "source": [
    "## Prompt Chaining\n",
    "\n",
    "* Breaking down a larger task into smaller, interconnected prompts. The output of each prompt then becomes the input for the next.\n",
    "\n",
    "* Improved accuracy: Smaller, focused prompts can lead to better results from the language model.\n",
    "* Debugging: It's easier to identify where things go wrong within the chain, allowing for targeted adjustments and improvements.\n",
    "* Complex tasks: By breaking down intricate problems into manageable steps, prompt chaining enables the language model to tackle more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace4575-919b-4773-9ceb-ed548ac50d61",
   "metadata": {},
   "source": [
    "## Iterative Generation\n",
    "\n",
    "Building the desired output iteratively. Can be used in combination with prompt chaining.\n",
    "\n",
    "* Longer outputs: It allows for the creation of longer and more detailed outputs, exceeding the limitations of a single generation window.\n",
    "* Flexibility: You can adjust and refine the output at each iteration, ensuring the story develops in the desired direction.\n",
    "* Human-in-the-loop control: You can provide feedback and guidance at each step, ensuring the story aligns with your creative vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8a6a8-6133-47d5-aa2f-1ce6d2b7f04a",
   "metadata": {},
   "source": [
    "As an example of prompt chaining and iterative generation, imagine writing a story with the LLM.\n",
    "\n",
    "You could prompt it for a premise, then use that to prompt for an outline, and then use that to prompt for a story.\n",
    "\n",
    "Then, you could use a \"continuation prompt\" -- essentially ask it to generate one part of the outline at a time, check the output, fix, and get it to generate the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4ba47-939f-4256-aabe-dc4c4ba138d4",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "\n",
    "LangChain is a framework designed to simplify the development of applications powered by LLMs.\n",
    "\n",
    "It provides tools to streamline the integration of LLMs with other services and enhance their capabilities, making it easier to create complex, data-driven applications.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "* Data Connection:\n",
    "  * Loading: Connect to various data sources, including local files, web pages, databases, and APIs.\n",
    "  * Querying: Use natural language to query and retrieve information from these sources.\n",
    "\n",
    "* Model Integration:\n",
    "  * LLM Wrappers: Simplify the usage of various language models by providing \n",
    "  * Chains: Create sequences of calls to LLMs and other tools, allowing for complex workflows and processes.\n",
    "\n",
    "* Agents:\n",
    "  * Action Selection: Automatically determine which action to take based on the input (e.g., querying a database, calling an API).\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0)\n",
    "\n",
    "write_query_chain = create_sql_query_chain(llm, db)\n",
    "\n",
    "validate_prompt = PromptTemplate(\n",
    "    input_variables=[\"not_formatted_query\"],\n",
    "    template=\"\"\"You are going to receive a text that contains a SQL query. Extract that query.\n",
    "    Make sure that it is a valid SQL command that can be passed directly to the Database.\n",
    "    Avoid using Markdown for this task.\n",
    "    Text: {not_formatted_query}\"\"\"\n",
    ")\n",
    "\n",
    "validate_chain = write_query_chain | validate_prompt | llm | StrOutputParser()\n",
    "validate_chain.invoke({\"question\": \"What is the total population?\"})\n",
    "\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "execute_chain = validate_chain | execute_query\n",
    "execute_chain.invoke({\"question\": \"What is the total population?\"})\n",
    "\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are going to receive a original user question, generated SQL query, and result of said query. You should use this information to answer the original question. Use only information provided to you.\n",
    "\n",
    "Original Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n",
    "answer_chain = (\n",
    "    RunnablePassthrough.assign(query=validate_chain).assign(\n",
    "        result=itemgetter(\"query\") | execute_query\n",
    "    )\n",
    "    | answer_prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer_chain.invoke({\"question\": \"What is the total population?\"})\n",
    "```\n",
    "> 'The total population is 29,421,840. \\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2282ffe-60f6-410a-a0ae-0d8cbf71849f",
   "metadata": {},
   "source": [
    "## Chroma\n",
    "\n",
    "Chroma is an open-source vector / embedding DB, specifically useful for ML applications.\n",
    "\n",
    "__Key Features:__\n",
    "\n",
    "* Search Engines\n",
    "* Recommendation Systems\n",
    "* Clustering and Classification\n",
    "* Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bed46d-3660-44d2-983f-c587822a81ad",
   "metadata": {},
   "source": [
    "## RAG Architecture With LangChain\n",
    "\n",
    "General RAG architecture consists of:\n",
    "\n",
    "* __Retriever__: Based on user query, gets relevant sections from the document, to be passed into the generator\n",
    "* __Generator__: The relevant snippets + query are passed into the LLM to get relevant answers\n",
    "\n",
    "Example:\n",
    "\n",
    "### Retriever\n",
    "\n",
    "1. Read and parse website data with LangChain\n",
    "\n",
    "2. Create embeddings of the website data (can use Gemini's embedding model)\n",
    "\n",
    "3. Store the embeddings in Chroma's vector store\n",
    "\n",
    "4. Create a Retriever frmo the Chroma vector store, to pass the relevant embeddings to the LLM, along with the query\n",
    "\n",
    "```python\n",
    "loader = WebBaseLoader(\"https://blog.google/technology/ai/google-gemini-ai/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Extract the text from the website data document\n",
    "text_content = docs[0].page_content\n",
    "\n",
    "# The text content between the substrings \"code, audio, image and video.\" to\n",
    "# \"Cloud TPU v5p\" is relevant for this tutorial. You can use Python's `split()`\n",
    "# to select the required content.\n",
    "text_content_1 = text_content.split(\"code, audio, image and video.\",1)[1]\n",
    "final_text = text_content_1.split(\"Cloud TPU v5p\",1)[0]\n",
    "\n",
    "# Convert the text to LangChain's `Document` format\n",
    "docs = [Document(page_content=final_text, metadata={\"source\": \"local\"})]\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Save to disk\n",
    "vectorstore = Chroma.from_documents(\n",
    "                     documents=docs,                 # Data\n",
    "                     embedding=gemini_embeddings,    # Embedding model\n",
    "                     persist_directory=\"./chroma_db\" # Directory to save data\n",
    "                     )\n",
    "\n",
    "# Load from disk\n",
    "vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"./chroma_db\",       # Directory of db\n",
    "                        embedding_function=gemini_embeddings   # Embedding model\n",
    "                   )\n",
    "# Get the Retriever interface for the store to use later.\n",
    "# When an unstructured query is given to a retriever it will return documents.\n",
    "# Read more about retrievers in the following link.\n",
    "# https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
    "#\n",
    "# Since only 1 document is stored in the Chroma vector store, search_kwargs `k`\n",
    "# is set to 1 to decrease the `k` value of chroma's similarity search from 4 to\n",
    "# 1. If you don't pass this value, you will get a warning.\n",
    "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Check if the retriever is working by trying to fetch the relevant docs related\n",
    "# to the word 'MMLU' (Massive Multitask Language Understanding). If the length is greater than zero, it means that\n",
    "# the retriever is functioning well.\n",
    "print(len(retriever.get_relevant_documents(\"MMLU\")))\n",
    "```\n",
    "\n",
    "### Generator\n",
    "\n",
    "* Chain together:\n",
    "  1. A prompt for extracting the relevant embeddings using the retriever\n",
    "  2. A prompt for answering questions using LangChain\n",
    "  3. An LLM model from Gemini for prompting\n",
    "* Run the created chain with a question as input\n",
    "\n",
    "```python\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# To configure model parameters use the `generation_config` parameter.\n",
    "# eg. generation_config = {\"temperature\": 0.7, \"topP\": 0.8, \"topK\": 40}\n",
    "# If you only want to set a custom temperature for the model use the\n",
    "# \"temperature\" parameter directly.\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
    "\n",
    "# Prompt template to query Gemini\n",
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use five sentences maximum and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "# Combine data from documents to readable string format.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create stuff documents chain using LCEL.\n",
    "#\n",
    "# This is called a chain because you are chaining together different elements\n",
    "# with the LLM. In the following example, to create the stuff chain, you will\n",
    "# combine the relevant context from the website data matching the question, the\n",
    "# LLM model, and the output parser together like a chain using LCEL.\n",
    "#\n",
    "# The chain implements the following pipeline:\n",
    "# 1. Extract the website data relevant to the question from the Chroma\n",
    "#    vector store and save it to the variable `context`.\n",
    "# 2. `RunnablePassthrough` option to provide `question` when invoking\n",
    "#    the chain.\n",
    "# 3. The `context` and `question` are then passed to the prompt where they\n",
    "#    are populated in the respective variables.\n",
    "# 4. This prompt is then passed to the LLM (`gemini-pro`).\n",
    "# 5. Output from the LLM is passed through an output parser\n",
    "#    to structure the model's response.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Gemini?\")\n",
    "```\n",
    "\n",
    "> \"Gemini is Google's largest and most capable AI model, designed to be highly flexible and efficient across various devices, from data centers to mobile devices. It's optimized in three sizes: Ultra, Pro, and Nano, catering to different complexity levels and task requirements. Gemini surpasses state-of-the-art performance on multiple benchmarks, including text, code, and multimodal tasks, showcasing its advanced reasoning abilities. This model is trained to understand and process information across various modalities, including text, images, audio, and more, making it ideal for complex tasks like coding and scientific research. \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a4e06-6b2e-4868-ab37-36cd4e28a0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
