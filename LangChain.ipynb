{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935d4516-06ed-4057-aea8-edfba85cd216",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64103cfb-0802-43d3-9395-f1f84b2b1dd3",
   "metadata": {},
   "source": [
    "_Heavily cribbed from https://github.com/gkamradt/langchain-tutorials/_\n",
    "\n",
    "## Concepts\n",
    "\n",
    "> LangChain is a framework for developing apps with language models.\n",
    "\n",
    "It makes development easier in two ways:\n",
    "\n",
    "1. __Integration__: Links external data, such as files, other apps, or APIs, with the LLM\n",
    "2. __Agency__: Allows LLMs to interact with its environment via decision making. Use LLMs to decide which action to take next.\n",
    "\n",
    "## References\n",
    "\n",
    "[Tutorials](https://python.langchain.com/v0.1/docs/additional_resources/tutorials/)\n",
    "[Use Cases](https://python.langchain.com/v0.1/docs/use_cases/)\n",
    "\n",
    "* Q&A with RAG\n",
    "* Extracting structured output\n",
    "* Chatbots\n",
    "* Tool use and agents\n",
    "* Query analysis\n",
    "* Q&A over SQL + CSV\n",
    "* More\n",
    "\n",
    "[Tool List](https://python.langchain.com/v0.1/docs/integrations/tools/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234717c-0628-4416-ad12-ca28165d0f3c",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "### 1. Schema: The Building Blocks for working with LLMs\n",
    "\n",
    "#### 1.1 Text\n",
    "\n",
    "#### 1.2 Chat Messages\n",
    "\n",
    "Like text, but with a message type:\n",
    "\n",
    "* System: Background context\n",
    "* Human\n",
    "* AI\n",
    "\n",
    "```python\n",
    "%pip install python-dotenv\n",
    "%pip install langchain\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key=os.getenv('OPENAI_API_KEY', 'YourAPIKey')\n",
    "\n",
    "%pip install openai\n",
    "%pip install langchain-community langchain-core\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# This it the language model we'll use. We'll talk about what we're doing below in the next section\n",
    "chat = ChatOpenAI(temperature=.7, openai_api_key=openai_api_key)\n",
    "```\n",
    "\n",
    "```\n",
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
    "        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "> AIMessage(content='You could try a caprese salad with fresh tomatoes, mozzarella, and basil.')\n",
    "\n",
    "You can also pass more chat history w/ responses from the AI\n",
    "\n",
    "```python\n",
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
    "        HumanMessage(content=\"I like the beaches where should I go?\"),\n",
    "        AIMessage(content=\"You should go to Nice, France\"),\n",
    "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
    "    ])\n",
    "```\n",
    "\n",
    "#### 1.3 Documents\n",
    "\n",
    "An object that holds text and metadata about the text.\n",
    "\n",
    "```python\n",
    "from langchain.schema import Document\n",
    "\n",
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"The LangChain Papers\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af88ece7-91db-4fd2-8955-27cb1e858346",
   "metadata": {},
   "source": [
    "### 2. Models\n",
    "\n",
    "The models interface to the AI brains.\n",
    "\n",
    "* __2.1 Language Model__: Text in --> Text out\n",
    "* __2.2 Chat Model__: Takes a series of messages --> return message output\n",
    "* __2.3 Function Calling Model__: Fine-tuned to give structured data output. Useful when making an API call to an external service or doing data extraction.\n",
    "```python\n",
    "chat = ChatOpenAI(model='gpt-3.5-turbo-0613', temperature=1, openai_api_key=openai_api_key)\n",
    "\n",
    "output = chat(messages=\n",
    "     [\n",
    "         SystemMessage(content=\"You are an helpful AI bot\"),\n",
    "         HumanMessage(content=\"What’s the weather like in Boston right now?\")\n",
    "     ],\n",
    "     functions=[{\n",
    "         \"name\": \"get_current_weather\",\n",
    "         \"description\": \"Get the current weather in a given location\",\n",
    "         \"parameters\": {\n",
    "             \"type\": \"object\",\n",
    "             \"properties\": {\n",
    "                 \"location\": {\n",
    "                     \"type\": \"string\",\n",
    "                     \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                 },\n",
    "                 \"unit\": {\n",
    "                     \"type\": \"string\",\n",
    "                     \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                 }\n",
    "             },\n",
    "             \"required\": [\"location\"]\n",
    "         }\n",
    "     }\n",
    "     ]\n",
    ")\n",
    "output\n",
    "```\n",
    "\n",
    "```\n",
    "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}})\n",
    "```\n",
    "\n",
    "* __2.4 Text Embedding__: Turns the text into a vector, useful when comparing text.\n",
    "\n",
    "```python\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "text = \"Hi! It's time for the beach\"\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b7781-305f-446b-86e1-018c13d45475",
   "metadata": {},
   "source": [
    "### 3. Prompts\n",
    "\n",
    "#### 3.1 Prompts\n",
    "\n",
    "What you pass to the underlying model\n",
    "\n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "# I like to use three double quotation marks for my prompts because it's easier to read\n",
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "print(llm(prompt))\n",
    "```\n",
    " \n",
    "#### 3.2 Prompt Templates\n",
    "\n",
    "An object that helps create prompts based on a combination of user input, other non-static information and a fixed template string.\n",
    "\n",
    "Think of it as an f-string in python but for prompts\n",
    "\n",
    "Advanced: Check out LangSmithHub(https://smith.langchain.com/hub) for many more communit prompt templates\n",
    "\n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Rome')\n",
    "\n",
    "print (f\"Final Prompt: {final_prompt}\")\n",
    "print (\"-----------\")\n",
    "print (f\"LLM Output: {llm(final_prompt)}\")\n",
    "```\n",
    "\n",
    "#### 3.3 Example Selectors\n",
    "\n",
    "An easy way to select from a series of examples that allow you to dynamic place in-context information into your prompt. Often used when your task is nuanced or you have a large list of examples.\n",
    "\n",
    "```python\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]\n",
    "\n",
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples, \n",
    "    \n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key), \n",
    "    \n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma, \n",
    "    \n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    ")\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "    \n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    \n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"],\n",
    ")\n",
    "\n",
    "# Select a noun!\n",
    "my_noun = \"plant\"\n",
    "# my_noun = \"student\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))\n",
    "\n",
    "```\n",
    "Give the location an item is usually found in\n",
    "\n",
    "Example Input: tree\n",
    "Example Output: ground\n",
    "\n",
    "Example Input: bird\n",
    "Example Output: nest\n",
    "\n",
    "Input: plant\n",
    "Output:\n",
    "```\n",
    "\n",
    "```python\n",
    "llm(similar_prompt.format(noun=my_noun))\n",
    "```\n",
    "\n",
    "\n",
    "`pot`\n",
    "\n",
    "\n",
    "#### 3.4 Output Parsers: Prompt Instructions & String Parsing\n",
    "\n",
    "A helpful way to format the output of a model. Usually used for structured output. LangChain has a bunch more output parsers listed on their documentation.\n",
    "\n",
    "1. __Format Instructions__: A autogenerated prompt that tells the LLM how to format it's response based off your desired result\n",
    "2. __Parser__: A method which will extract your model's text output into a desired structure (usually json)\n",
    "\n",
    "##### 3.5 Output Parsers: OpenAI Functions\n",
    "\n",
    "When OpenAI released function calling, the game changed. This is recommended method when starting out.\n",
    "\n",
    "They trained models specifically for outputing structured data. It became super easy to specify a Pydantic schema and get a structured output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26357e5-8e8e-4fe6-9e04-63911ad0d42e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4. Indexes\n",
    "\n",
    "Indexes are used to structure documents so LLMs can work with them.\n",
    "\n",
    "#### 4.1 Document Loaders\n",
    "\n",
    "Allow you to import documents from other sources. E.g., hacker news, wiki, web pages.\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "urls = [\n",
    "    \"http://www.paulgraham.com/\",\n",
    "]\n",
    "\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "data[0].page_content\n",
    "```\n",
    "\n",
    "#### 4.2 Text Splitters\n",
    "\n",
    "Often times your document is too long (like a book) for your LLM or Vector DB. You need to split it up into chunks. Text splitters help with this.\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# This is a long document we can split up.\n",
    "with open('data/PaulGrahamEssays/worked.txt') as f:\n",
    "    pg_work = f.read()\n",
    "    \n",
    "# 1 Document\n",
    "print (f\"You have {len([pg_work])} document\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap  = 20,\n",
    ")\n",
    "\n",
    "# 610 Documents\n",
    "texts = text_splitter.create_documents([pg_work])\n",
    "```\n",
    "\n",
    "#### 4.3 Retrievers\n",
    "\n",
    "An easy way to combine documents with large language models.\n",
    "\n",
    "There are many different types of retrievers, the most widely supported is the VectoreStoreRetriever\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Embedd your texts\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# Init your retriever. Asking for just 1 document back\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "docs = retriever.get_relevant_documents(\"what types of things did the author want to build?\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))\n",
    "```\n",
    "\n",
    "#### 4.4 VectorStores\n",
    "Databases to store vectors. Most popular ones are [Pinecone](https://www.pinecone.io/) & [Weaviate](https://weaviate.io/). More examples on OpenAIs [retriever documentation](https://github.com/openai/chatgpt-retrieval-plugin#choosing-a-vector-database). [Chroma](https://www.trychroma.com/) & [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) are easy to work with locally.\n",
    "\n",
    "Conceptually, think of them as tables w/ a column for embeddings (vectors) and a column for metadata.\n",
    "\n",
    "Example\n",
    "\n",
    "| Embedding      | Metadata |\n",
    "| ----------- | ----------- |\n",
    "| [-0.00015641732898075134, -0.003165106289088726, ...]      | {'date' : '1/2/23}       |\n",
    "| [-0.00035465431654651654, 1.4654131651654516546, ...]   | {'date' : '1/3/23}        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d31c3-01c7-4e4d-827d-32ecd3640704",
   "metadata": {},
   "source": [
    "### 5. Memory\n",
    "\n",
    "Helping LLMs remember information.\n",
    "\n",
    "Memory is a bit of a loose term. It could be as simple as remembering information you've chatted about in the past or more complicated information retrieval.\n",
    "\n",
    "There are many types of memory, explore [the documentation](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html) to see which one fits your use case.\n",
    "\n",
    "#### 5.1 Chat Message History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddad1c7-0f8b-44fd-9e61-0b96a0c9f71a",
   "metadata": {},
   "source": [
    "### 6. Chains\n",
    "\n",
    "Combining different LLM calls and action automatically\n",
    "\n",
    "Ex: Summary #1, Summary #2, Summary #3 > Final Summary\n",
    "\n",
    "Check out [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo&t=2s) explaining different summarization chain types\n",
    "\n",
    "There are [many applications of chains](https://python.langchain.com/en/latest/modules/chains/how_to_guides.html) search to see which are best for your use case.\n",
    "\n",
    "#### 6.1 Simple Sequential Chains\n",
    "\n",
    "Easy chains where you can use the output of an LLM as an input into another. Good for breaking up tasks (and keeping your LLM focused)\n",
    "\n",
    "\n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = OpenAI(temperature=1, openai_api_key=openai_api_key)\n",
    "\n",
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)\n",
    "\n",
    "review = overall_chain.run(\"Rome\")\n",
    "```\n",
    "\n",
    "```\n",
    "> Entering new SimpleSequentialChain chain...\n",
    "\n",
    "A classic dish from Rome is Spaghetti alla Carbonara, featuring egg, Parmesan cheese, black pepper, and pancetta or guanciale.\n",
    "\n",
    "Ingredients:\n",
    "- 8oz spaghetti \n",
    "- 4 tablespoons olive oil\n",
    "- 4oz diced pancetta or guanciale\n",
    "- 2 cloves garlic, minced\n",
    "- 2 eggs, lightly beaten\n",
    "- 2 tablespoons parsley, chopped \n",
    "- ½ cup grated Parmesan \n",
    "- Salt and black pepper to taste\n",
    "\n",
    "Instructions:\n",
    "1. Bring a pot of salted water to a boil and add the spaghetti. Cook according to package directions. \n",
    "2. Meanwhile, add the olive oil to a large skillet over medium-high heat. Add the diced pancetta and garlic, and cook until pancetta is browned and garlic is fragrant.\n",
    "3. In a medium bowl, whisk together the eggs, parsley, Parmesan, and salt and pepper.\n",
    "4. Drain the cooked spaghetti and add it to the skillet with the pancetta and garlic. Remove from heat and pour the egg mixture over the spaghetti, stirring to combine. \n",
    "5. Serve the spaghetti alla carbonara with additional Parmesan cheese and black pepper.\n",
    "\n",
    "> Finished chain.\n",
    "```\n",
    "\n",
    "#### 6.2 Summarization Chain\n",
    "\n",
    "Easily run through long numerous documents and get a summary. Check out [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo) for other chain types besides map-reduce.\n",
    "\n",
    "```python\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/disc.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ccb390-2679-43c7-88a8-6dd03a8b1602",
   "metadata": {},
   "source": [
    "### 7. Agents\n",
    "\n",
    "Official LangChain Documentation describes agents:\n",
    "\n",
    "> Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an **unknown chain** that depends on the user's input. In these types of chains, there is a “agent” which has access to a suite of tools. Depending on the user input, the agent can then **decide which, if any, of these tools to call**.\n",
    "\n",
    "\n",
    "Basically you use the LLM not just for text output, but also for decision making. The coolness and power of this functionality can't be overstated enough.\n",
    "\n",
    "#### 7.1 Agents\n",
    "\n",
    "The language model that drives decision making.\n",
    "\n",
    "Takes an input and returns a response corresponding to an action to take along with an action input.\n",
    "\n",
    "You can see different types of agents (which are better for different use cases) [here](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html).\n",
    "\n",
    "#### 7.2 Tools\n",
    "\n",
    "The capability of an agent. An abstraction on top of a function that makes it easy for LLMs to interact with it. E.g. Google Search.\n",
    "\n",
    "##### 7.3 Toolkit\n",
    "\n",
    "A group of tools that your agent can select from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f7a75-d611-4e73-ac40-75f230d83019",
   "metadata": {},
   "source": [
    "## LangChain Use Cases\n",
    "\n",
    "### Main Use Cases\n",
    "\n",
    "1. __Summarization__\n",
    "2. __Document Q&A__\n",
    "3. __Extraction__: Pull structured data out of a body of text or query\n",
    "4. __Evaluation__: Understand the quality of output from your application\n",
    "5. __Query Data__: Pull data from DBs or other tabular sources\n",
    "6. __Code Understanding__\n",
    "7. __Interact with APIs__\n",
    "8. __Chatbots__\n",
    "9. __Agents__\n",
    "\n",
    "[LangChain Project Gallery](https://github.com/gkamradt/langchain-tutorials) for examples of these use cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c323b0-8161-4813-a2ff-1e5922e34e69",
   "metadata": {},
   "source": [
    "### 1. Summarization\n",
    "\n",
    "You can pass in short text via a prompt:\n",
    "\n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Note, the default model is already 'text-davinci-003' but I call it out here explicitly so you know where to change it later if you want\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003', openai_api_key=openai_api_key)\n",
    "\n",
    "# Create our template\n",
    "template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Create a LangChain prompt template that we can insert values to later\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(text=\"Some text to summarize\")\n",
    "\n",
    "output = llm(final_prompt)\n",
    "print (output)\n",
    "```\n",
    "\n",
    "You can also split up longer text into documents, and then summarize all of them.\n",
    "\n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "with open('data/PaulGrahamEssays/good.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "# Note: Other text splitters exist.\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=5000, chunk_overlap=350)\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "# Get your chain ready to use\n",
    "chain = load_summarize_chain(llm=llm, chain_type='map_reduce') # verbose=True optional to see what is getting sent to the LLM\n",
    "\n",
    "# Use it. This will run through the 4 documents, summarize the chunks, then get a summary of the summary.\n",
    "output = chain.run(docs)\n",
    "print (output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2677a2b-8b73-4cb8-92c1-4d9fd32d319f",
   "metadata": {},
   "source": [
    "### 2. Document Q&A\n",
    "\n",
    "In order to use LLMs for question and answer we must:\n",
    "\n",
    "1. Pass the LLM relevant context it needs to answer a question\n",
    "2. Pass it our question that we want answered\n",
    "\n",
    "Simplified, this process looks like this \"llm(your context + your question) = your answer\"\n",
    "\n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "context = \"\"\"\n",
    "Rachel is 30 years old\n",
    "Bob is 45 years old\n",
    "Kevin is 65 years old\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who is under 40 years old?\"\n",
    "\n",
    "output = llm(context + question)\n",
    "\n",
    "# I strip the text to remove the leading and trailing whitespace\n",
    "print (output.strip())\n",
    "```\n",
    "\n",
    "The hard part comes in when you need to be selective about *which* data you put in your context. This field of study is called \"[document retrieval](https://python.langchain.com/en/latest/modules/indexes/retrievers.html)\" and tightly coupled with AI Memory.\n",
    "\n",
    "#### 2.1 Using Embeddings\n",
    "\n",
    "The high-level VectorStore process:\n",
    "\n",
    "* Split your text\n",
    "* Embed the chunks\n",
    "* Put embeddings in a DB\n",
    "* Query them\n",
    "\n",
    "For a full video on this check out [How To Question A Book](https://www.youtube.com/watch?v=h0DHDp1FbmQ)\n",
    "\n",
    "The goal is to select relevant chunks of our long text, but which chunks do we pull? The most popular method is to pull similar texts based off comparing vector embeddings.\n",
    "\n",
    "```python\n",
    "from langchain import OpenAI\n",
    "\n",
    "# The vectorstore we'll be using\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# The LangChain component we'll use to get the documents\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# The easy document loader for text\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# The embedding engine that will convert our text to vectors\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "doc = loader.load()\n",
    "\n",
    "# Split the doc into smaller pieces\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "\n",
    "num_total_characters = sum([len(x.page_content) for x in docs])\n",
    "\n",
    "# Get your embeddings engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Embed your documents and combine with the raw text in a pseudo db. Note: This will make an API call to OpenAI\n",
    "docsearch = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Create retrieval engine\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())\n",
    "\n",
    "query = \"What does the author describe as good work?\"\n",
    "qa.run(query)\n",
    "```\n",
    "\n",
    "If you wanted to go further, you could hook up the data to a cloud Vector DB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64319cc3-4dda-4194-b5d6-e5adf601868d",
   "metadata": {},
   "source": [
    "### 3. Extraction\n",
    "\n",
    "Parse some data from a piece of text. Often used to structure the data. This could be extracting data from a document to insert into a DB, or extract params from a user request to make an API call.\n",
    "\n",
    "A popular library for extraction is [Kor](https://eyurtsev.github.io/kor/).\n",
    "\n",
    "#### 3.1 Simple Extraction\n",
    "\n",
    "```python\n",
    "# To help construct our Chat Messages\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# We will be using a chat model, defaults to gpt-3.5-turbo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# To parse outputs and get structured data back\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo', openai_api_key=openai_api_key)\n",
    "\n",
    "instructions = \"\"\"\n",
    "You will be given a sentence with fruit names, extract those fruit names and assign an emoji to them\n",
    "Return the fruit name and emojis in a python dictionary\n",
    "\"\"\"\n",
    "\n",
    "fruit_names = \"\"\"\n",
    "Apple, Pear, this is an kiwi\n",
    "\"\"\"\n",
    "# Make your prompt which combines the instructions w/ the fruit names\n",
    "prompt = (instructions + fruit_names)\n",
    "\n",
    "# Call the LLM\n",
    "output = chat_model([HumanMessage(content=prompt)])\n",
    "\n",
    "output_dict = eval(output.content)\n",
    "\n",
    "> {'Apple': '🍎', 'Pear': '🍐', 'kiwi': '🥝'}\n",
    "\n",
    "```\n",
    "\n",
    "#### 3.2 LangChain's Response Schema\n",
    "\n",
    "LangChain's response schema will does two things for us:\n",
    "\n",
    "Autogenerate the a prompt with bonafide format instructions. This is great because I don't need to worry about the prompt engineering side, I'll leave that up to LangChain!\n",
    "\n",
    "Read the output from the LLM and turn it into a proper python object for me.\n",
    "\n",
    "```python\n",
    "# The schema I want out\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
    "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
    "]\n",
    "\n",
    "# The parser that will look for the LLM output in my schema and return it back to me\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# The prompt template that brings it all together\n",
    "# Note: This is a different prompt template than before because we are using a Chat Model\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"Given a command from the user, extract the artist and song names \\n \\\n",
    "                                                    {format_instructions}\\n{user_prompt}\")  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "fruit_query = prompt.format_prompt(user_prompt=\"I really like So Young by Portugal. The Man\")\n",
    "print (fruit_query.messages[0].content)\n",
    "\n",
    "fruit_output = chat_model(fruit_query.to_messages())\n",
    "output = output_parser.parse(fruit_output.content)\n",
    "```\n",
    "\n",
    "> {'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bfc29c-2147-4b62-a6b0-5f21406fe815",
   "metadata": {},
   "source": [
    "### 4. Evaluation\n",
    "\n",
    "Evaluation is the process of doing quality checks on the output of your applications. Normal, deterministic, code has tests we can run, but judging the output of LLMs is more difficult because of the unpredictableness and variability of natural language. LangChain provides tools that aid us in this journey.\n",
    "\n",
    "E.g. Run quality check on summarization or Q&A pipeline answers.\n",
    "\n",
    "```python\n",
    "# Embeddings, store, and retrieval\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Model and doc loader\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Eval!\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Our long essay from before\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "doc = loader.load()\n",
    "\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "\n",
    "# Embeddings and docstore\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docsearch = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# input_key is the dict key below\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(), input_key=\"question\")\n",
    "\n",
    "question_answers = [\n",
    "    {'question' : \"Which company sold the microcomputer kit that his friend built himself?\", 'answer' : 'Healthkit'},\n",
    "    {'question' : \"What was the small city he talked about in the city that is the financial capital of USA?\", 'answer' : 'Yorkville, NY'}\n",
    "]\n",
    "\n",
    "predictions = chain.apply(question_answers)\n",
    "predictions\n",
    "\n",
    "> [{'question': 'Which company sold the microcomputer kit that his friend built himself?',\n",
    "  'answer': 'Healthkit',\n",
    "  'result': ' The microcomputer kit was sold by Heathkit.'},\n",
    " {'question': 'What was the small city he talked about in the city that is the financial capital of USA?',\n",
    "  'answer': 'Yorkville, NY',\n",
    "  'result': ' The small city he talked about is New York City, which is the financial capital of the United States.'}]\n",
    "\n",
    "# Now let's have the LLM grade itself\n",
    "\n",
    "# Start your eval chain\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "# Have it grade itself. The code below helps the eval_chain know where the different parts are\n",
    "graded_outputs = eval_chain.evaluate(question_answers,\n",
    "                                     predictions,\n",
    "                                     question_key=\"question\",\n",
    "                                     prediction_key=\"result\",\n",
    "                                     answer_key='answer')\n",
    "\n",
    "graded_outputs\n",
    "\n",
    "> [{'text': ' CORRECT'}, {'text': ' INCORRECT'}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440065a-2342-4d43-8a48-c1d512f3579c",
   "metadata": {},
   "source": [
    "### 5. Querying Data\n",
    "\n",
    "For futher reading check out \"Agents + Tabular Data\" ([Pandas](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/pandas.html), [SQL](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/sql_database.html), [CSV](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/csv.html))\n",
    "\n",
    "```python\n",
    "from langchain import OpenAI, SQLDatabase, SQLDatabaseChain\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "sqlite_db_path = 'data/San_Francisco_Trees.db'\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{sqlite_db_path}\")\n",
    "\n",
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)\n",
    "\n",
    "db_chain.run(\"How many Species of trees are there in San Francisco?\")\n",
    "\n",
    "> > Entering new SQLDatabaseChain chain...\n",
    "How many Species of trees are there in San Francisco?\n",
    "SQLQuery:SELECT COUNT(DISTINCT \"qSpecies\") FROM \"SFTrees\";\n",
    "SQLResult: [(578,)]\n",
    "Answer:There are 578 Species of trees in San Francisco.\n",
    "> Finished chain.\n",
    "```\n",
    "\n",
    "There are quite a few steps going on here:\n",
    "\n",
    "1. Find which table to use\n",
    "2. Find which column to use\n",
    "3. Construct the correct SQL\n",
    "4. Execute the query\n",
    "5. Get the result\n",
    "6. Return the natural language response back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c0e7b-adaa-40e0-9a4e-82d1bf79d0de",
   "metadata": {},
   "source": [
    "### 6. Code Understanding\n",
    "\n",
    "We just load these up as docs.\n",
    "\n",
    "```python\n",
    "# Helper to read local files\n",
    "import os\n",
    "\n",
    "# Vector Support\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Model and chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Text splitters\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', openai_api_key=openai_api_key)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(disallowed_special=(), openai_api_key=openai_api_key)\n",
    "\n",
    "root_dir = 'data/thefuzz'\n",
    "docs = []\n",
    "\n",
    "# Load each file as a doc...\n",
    "\n",
    "# Go through each folder\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    \n",
    "    # Go through each file\n",
    "    for file in filenames:\n",
    "        try: \n",
    "            # Load up the file as a doc and split\n",
    "            loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "        except Exception as e: \n",
    "            pass\n",
    "        \n",
    "# Embed and store in a docstore\n",
    "docsearch = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Get our retriever ready\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())\n",
    "\n",
    "query = \"What function do I use if I want to find the most similar item in a list of items?\"\n",
    "output = qa.run(query)\n",
    "\n",
    "> You can use the `process.extractOne()` function from `thefuzz` package to find the most similar item in a list of items. Here's an example:\n",
    "\n",
    "query = \"Can you write the code to use the process.extractOne() function? Only respond with code. No other text or explanation\"\n",
    "output = qa.run(query)\n",
    "\n",
    "> code...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e0660-82c0-42eb-aa35-fea435a5394f",
   "metadata": {},
   "source": [
    "### 7. Interacting With APIs\n",
    "\n",
    "This is closely related to Agents and Plugins.\n",
    "\n",
    "LangChain's APIChain can read API documentation and understand which endpoint it needs to call.\n",
    "\n",
    "```python\n",
    "from langchain.chains import APIChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "\n",
    "api_docs = \"\"\"\n",
    "\n",
    "BASE URL: https://restcountries.com/\n",
    "\n",
    "API Documentation:\n",
    "\n",
    "The API endpoint /v3.1/name/{name} Used to find informatin about a country. All URL parameters are listed below:\n",
    "    - name: Name of country - Ex: italy, france\n",
    "    \n",
    "The API endpoint /v3.1/currency/{currency} Uesd to find information about a region. All URL parameters are listed below:\n",
    "    - currency: 3 letter currency. Example: USD, COP\n",
    "    \n",
    "Woo! This is my documentation\n",
    "\"\"\"\n",
    "\n",
    "chain_new = APIChain.from_llm_and_api_docs(llm, api_docs, verbose=True)\n",
    "\n",
    "chain_new.run('Can you tell me information about france?')\n",
    "\n",
    "> Entering new APIChain chain...\n",
    " https://restcountries.com/v3.1/name/france\n",
    "> ...\n",
    "> France is an officially-assigned, ...  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619aa7d-8618-481b-9f89-21695ac7684d",
   "metadata": {},
   "source": [
    "### 8. Chatbots\n",
    "\n",
    "Again, memory becomes important. There are a ton of different [types of memory](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html), tinker to see which is best for you.\n",
    "\n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Chat specific components\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "template = \"\"\"\n",
    "You are a chatbot that is unhelpful.\n",
    "Your goal is to not help the user but only make jokes.\n",
    "Take what the user is saying and make a joke out of it\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=OpenAI(openai_api_key=openai_api_key), \n",
    "    prompt=prompt, \n",
    "    verbose=True, \n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "llm_chain.predict(human_input=\"Is an pear a fruit or vegetable?\")\n",
    "\n",
    "> ...\n",
    "\n",
    "llm_chain.predict(human_input=\"What was one of the fruits I first asked you about?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7cc958-6526-4e70-94c0-6feb657de21e",
   "metadata": {},
   "source": [
    "### 9. Agents\n",
    "\n",
    "Agents are the decision makers that can look a data, reason about what the next action should be, and execute that action for you via tools.\n",
    "\n",
    "Examples of advanced uses of agents appear in [BabyAGI](https://github.com/yoheinakajima/babyagi) and [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT)\n",
    "\n",
    "examples of what you can do with AutoGPT:\n",
    "\n",
    "1. Reddit Marketing Agent\n",
    "\n",
    "* This agent reads comments on Reddit.\n",
    "* It looks for people asking about your product.\n",
    "* It then automatically responds to them.\n",
    "\n",
    "2. YouTube Content Repurposing Agent\n",
    "\n",
    "* This agent subscribes to your YouTube channel.\n",
    "* When you post a new video, it transcribes it.\n",
    "* It uses AI to write a search engine optimized blog post.\n",
    "* Then, it publishes this blog post to your Medium account.\n",
    "\n",
    "```python\n",
    "# Helpers\n",
    "import os\n",
    "import json\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Agent imports\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# Tool imports\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.utilities import TextRequestsWrapper\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "search = GoogleSearchAPIWrapper(google_api_key=GOOGLE_API_KEY, google_cse_id=GOOGLE_CSE_ID)\n",
    "\n",
    "requests = TextRequestsWrapper()\n",
    "\n",
    "toolkit = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to search google to answer questions about current events\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name = \"Requests\",\n",
    "        func=requests.get,\n",
    "        description=\"Useful for when you to make a request to a URL\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create your agent by giving it 1. tools, 2. the LLM, and 3. What kind of agent it should be\n",
    "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)\n",
    "\n",
    "response = agent({\"input\":\"What is the capital of canada?\"})\n",
    "response['output']\n",
    "\n",
    "> > Entering new AgentExecutor chain...\n",
    " I need to find out what the capital of Canada is.\n",
    "Action: Search\n",
    "Action Input: \"capital of Canada\"\n",
    "Observation: Looking to build credit or earn rewards? Compare our rewards, Guaranteed secured and other Guaranteed credit cards. Canada's capital is Ottawa and its three largest metropolitan areas are Toronto, Montreal, and Vancouver. Canada. A vertical triband design (red, white, red) ... Browse available job openings at Capital One - CA. ... Together, we will build one of Canada's leading information-based technology companies – join us, ... Ottawa is the capital city of Canada. It is located in the southern portion of the province of Ontario, at the confluence of the Ottawa River and the Rideau ... Shopify Capital offers small business funding in the form of merchant cash advances to eligible merchants in Canada. If you live in Canada and need ... Download Capital One Canada and enjoy it on your iPhone, iPad and iPod touch. ... Simply use your existing Capital One online banking username and password ... A leader in the alternative asset space, TPG was built for a distinctive approach, managing assets through a principled focus on innovation. We're Canada's largest credit union by membership because we prioritize people, not profits. Let's build the right plan to reach your financial goals, together. The national capital is Ottawa, Canada's fourth largest city. It lies some 250 miles (400 km) northeast of Toronto and 125 miles (200 km) west of Montreal, ... Finding Value Across the Capital Structure: Limited Recourse Capital Notes. Limited Recourse Capital Notes are an evolving segment of the Canadian fixed-income ...\n",
    "Thought: I now know the final answer\n",
    "Final Answer: Ottawa is the capital of Canada.\n",
    "\n",
    "response = agent({\"input\":\"Tell me what the comments are about on this webpage https://news.ycombinator.com/item?id=34425779\"})\n",
    "response['output']\n",
    "\n",
    "> > Entering new AgentExecutor chain...\n",
    " I need to find out what the comments are about\n",
    "Action: Search\n",
    "Action Input: \"comments on https://news.ycombinator.com/item?id=34425779\"\n",
    "Observation: About a month after we started Y Combinator we came up with the phrase that ... Action Input: \"comments on https://news.ycombinator.com/item?id=34425779\" .\n",
    "Thought: I now know the comments are about Y Combinator\n",
    "Final Answer: The comments on the webpage are about Y Combinator.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2de2ce-0479-44f3-be32-15bbc7df3a6d",
   "metadata": {},
   "source": [
    "# Conceptual Guide\n",
    "\n",
    "From [official docs](https://python.langchain.com/v0.2/docs/concepts/).\n",
    "\n",
    "## LCEL\n",
    "\n",
    "LangChain Expression Language: A declarative way to chain LangChain components.\n",
    "\n",
    "## Runnable interface\n",
    "\n",
    "Many LangChain components implement `Runnable`:\n",
    "\n",
    "* `stream`: Stream back chunks\n",
    "* `invoke`: Call the chain on an input\n",
    "* `batch`: Call the chain on a list of insputs\n",
    "\n",
    "(These also include async versions)\n",
    "\n",
    "## Components\n",
    "\n",
    "### 1. Chat models\n",
    "\n",
    "Language models that use a sequence of messages as inputs and return chat messages as outputs.\n",
    "\n",
    "### 2. LLMs\n",
    "\n",
    "Language models that take a string as input and return a string. These are generally older models, and newer models are generally chat models.\n",
    "\n",
    "### 3. Messages\n",
    "\n",
    "Some language models take a list of messages as input and return a message.\n",
    "\n",
    "### 4. Prompt templates\n",
    "\n",
    "Helps translate user input and parameters into an instruction for the language model.\n",
    "\n",
    "Input is a dictionary, where each key is a variable for the prompt template to fill in.\n",
    "\n",
    "Output is a PromptValue.\n",
    "\n",
    "```python\n",
    "\n",
    "# String PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "# ChatPromptTemplate\n",
    "# Used to format a list of messages.\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})\n",
    "```\n",
    "\n",
    "### 5. Example Selectors\n",
    "\n",
    "Let's you select and format examples to pass into prompts.\n",
    "\n",
    "### 6. Output Parsers\n",
    "\n",
    "Takes the output from a model and formats it.\n",
    "\n",
    "E.g.,\n",
    "\n",
    "* JSON\n",
    "* XML\n",
    "* CSV\n",
    "\n",
    "### 7. Chat History\n",
    "\n",
    "Can wrap an arbitrary chain to keep track of inputs and outputs of the underlying chain, and append them as messages to a message database.\n",
    "\n",
    "They can then be loaded and passed into the chain as part of the input.\n",
    "\n",
    "### 8. Documents\n",
    "\n",
    "Information about some data.\n",
    "\n",
    "### 9. Document Loaders\n",
    "\n",
    "Allows you to load data from data sources such as Google Drive, CSV files, etc.\n",
    "\n",
    "### 10. Text Splitters\n",
    "\n",
    "Splits a document into smaller chunks that can fit into a context window.\n",
    "\n",
    "See the relevant [how-to guides](https://python.langchain.com/v0.2/docs/how_to/#text-splitters): there are nuances as we want to keep semantically related pieces of text together.\n",
    "\n",
    "### 11. Embedding Models\n",
    "\n",
    "Create a vector representation of some text.\n",
    "\n",
    "### 12. Vector Stores\n",
    "\n",
    "A way to store embedding vectors.\n",
    "\n",
    "### 13. Retrievers\n",
    "\n",
    "Returns documents given an unstructured query. More general than a vector store.\n",
    "\n",
    "Does not need to be able to store documents, only return them. A retreiver can be created from vector stores, but is more general than a vector store. For example, can be Wikipedia search.\n",
    "\n",
    "### 14. Tools\n",
    "\n",
    "Tools are utilities to be called by a model.\n",
    "\n",
    "E.g., could be a call to an external API.\n",
    "\n",
    "### 15. Toolkits\n",
    "\n",
    "A collection of tools designed to be used together for specific tasks.\n",
    "\n",
    "### 16. Agents\n",
    "\n",
    "Systems that use LLMs are a reasoning agent to determine actions to take, and what those intput should be.\n",
    "\n",
    "LangGraph is a LangChain extension aimed at creating highly controllable agents.\n",
    "\n",
    "#### 16.1 ReAct Agents\n",
    "\n",
    "Reason and Act agents:\n",
    "\n",
    "* \"Think\" what step to take\n",
    "* Choose an action from available tools\n",
    "* Generate arguments to that tool\n",
    "* Call the tool with generated arguments\n",
    "* Return the tool results back as an observation\n",
    "* Repeat until complete\n",
    "\n",
    "## Techniques\n",
    "\n",
    "### Retrieval (RAG)\n",
    "\n",
    "Techniques for RAG can be split up into the following:\n",
    "\n",
    "__1. Query translation__: Use an LLM to review and optionally modify the input\n",
    "__2. Routing__: Use an LLM to decide which data source to route the query to\n",
    "__3. Query construction__: Convert user input into query syntax\n",
    "__4. Indexing__: How you index your documents/text that is being retrieved\n",
    "__5. Search Improvement__: Improve similarity searching. E.g. include keyword search as well as semantic similarity.\n",
    "__6. Post Processing__: E.g. Rank or compress the documents you found that match.\n",
    "__7. Generation__: E.g. check for errors or search the web if no relevant documents found.\n",
    "\n",
    "(also see retrieval image below)\n",
    "\n",
    "#### Retrieval: Query Translation\n",
    "\n",
    "Use an LLM to review and optionally modify the input. This optimizes the raw user inputs for the retrieval system.\n",
    "\n",
    "For example, this could be extracting keywords, or generating multiple sub-questions.\n",
    "\n",
    "* __Multi-query__: Rewrite the question from multiple perspectives\n",
    "* __Decomposition__: Break the question into multiple smaller subproblems\n",
    "* __Step-back__: When a higher level understanding is required, ask the LLM to ask a generic step-back question about higher-level concepts, to ground the answer\n",
    "* __HyDE__: Use an LLM to convert questions into hypothetical documents that answer the question, and then retrieve real documents with the premise of doc-doc similarity\n",
    "\n",
    "#### Retrieval: Routing\n",
    "\n",
    "Use the LLM to review the input and route the query to the correct data source in the application.\n",
    "\n",
    "* __Logical routing__: Prompt the LLM to reason using rules to decide where to route the input\n",
    "* __Semantic routing__: Embeds the query and set of prompts and then chooses the appropriate prompt based on similarity\n",
    "\n",
    "#### Retrieval: Query Construction\n",
    "\n",
    "Convert the LLM input from natural language into query syntax\n",
    "\n",
    "* __Text to SQL__\n",
    "* __Text to Cypher__: If asking questions from data in a graph database\n",
    "* __Self query__: If asking questions that are better answered by fetching documents based on metadata rather than text similarity. Transforms user input into 1) a string to look up semantically and 2) a METADATA filter to go alongwith it (oftentimes queries are about metadata rather than document content itself)\n",
    "\n",
    "#### Retrieval: Indexing\n",
    "\n",
    "The design of the document index for retrieval. A powerful idea is to __decouple the documents that you index for retrieval from the documents you pass to the LLM for generation__.\n",
    "\n",
    "We chunk documents, but chunk size is hard to get right and affects results if they do not provide full context for the LLM.\n",
    "\n",
    "Below are some techniques:\n",
    "\n",
    "* __Vector store__: Quick and easy, create embeddings for each piece of text\n",
    "* __ParentDocument__: Vector store + document store. If your pages have lots of smaller piece of distinct info that is best indexed individually, but retrieved all together. Index multiple chunks for each document, then once found the matching chunk, return the whole document that chunk belongs to.\n",
    "* __Multi Vector__: Vector store + document store. If you are able to extract info from the documents that is more relevant than the text itself. E.g., create a summary of the document or Q&A of the document and index that, but then return the whole document.\n",
    "* __Time-Weighted Vector store__: If you have timestamps associated with your documents and want to retrieve the most recent ones.\n",
    "\n",
    "#### Retrieval: Improving Similarity Search\n",
    "\n",
    "In some cases, irrelevant content can dilute semantic usefulness of the embedding.\n",
    "\n",
    "(Will skip for now, but consider: embeddings are good at semantic search, but may not work well for keyword-based searching. Hybrid search is often available in vector stores and can improve vector search. Other techniques include ColBERT and MMR.)\n",
    "\n",
    "#### Retrieval: Post-processing\n",
    "\n",
    "Filtering and ranking retrieved documents. Very useful if you are combining documents returned from multiple sources, so you can down-rank less relevant docs or compress similar docs.\n",
    "\n",
    "* __Contextual Compression__: When retieved docs contain too much irrelevant info. Puts a post-processing step and extracts only the most relevant info from the documents.\n",
    "* __Ensemble__: If you have multiple retrieval methods and want to combine them. Fetches docs from multiple retrievers and combines them.\n",
    "* __Re-ranking__: Rank the documents based on relevant.\n",
    "\n",
    "#### Retrieval: Generation\n",
    "\n",
    "Building self-correction into the RAG system.\n",
    "\n",
    "Low quality systems can suffer from hallucinations or low quality retrieval (if a question is outside of the domain for the index). You can try to detect or self-correct these errors.\n",
    "\n",
    "This is a relatively new area.\n",
    "\n",
    "* __Self-RAG__: Fix answers with hallucinations or irrelvant content, check the doc during the answer generation flow, iteratively building an answer and self-correcting errors.\n",
    "* __Corrective-RAG__: When needed for a fallback for low relevant docs. E.g. do a fallback to web search if docs are not relevant to the query.\n",
    "\n",
    "\n",
    "### Text Splitting\n",
    "\n",
    "There are various types of text splitters, some of which add metadata, some of which don't.\n",
    "\n",
    "* __Recursive__: Splits on user defined characters. Tries to keep related pieces of text next to each other. Recommended way to start.\n",
    "* __HTML__: Splits on HTML characters. Add relevant information about where the chunk came from.\n",
    "* __Markdown__: Splits on Markdown characters. \n",
    "* __Code__\n",
    "* __Tokens__: Splits on tokens. A few exist with different ways to measure tokens.\n",
    "* __Character__: Splits on user defined character. Simple.\n",
    "* __Semantic Chunker _(experimental)___: Tries to split on sentences. Then combines next to each other if they are semantically similar enough.\n",
    "* __AI21 Semantic__: Identifies distinct topics that form coherent pieces of text and splits along those.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "Assessing the performance of the LLM-powered app.\n",
    "\n",
    "Involves testing the model's responses against a set of predefined criteria or benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c49c71-16dd-411e-81fe-b383e4c80160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://python.langchain.com/v0.2/assets/images/rag_landscape-627f1d0fd46b92bc2db0af8f99ec3724.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url=\"https://python.langchain.com/v0.2/assets/images/rag_landscape-627f1d0fd46b92bc2db0af8f99ec3724.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc100416-71c0-4fc8-a1e1-0d01631e5be6",
   "metadata": {},
   "source": [
    "### Multivector Retrieval\n",
    "\n",
    "Documents might consist of text, tables, and images. If we want to search these with RAG, we can turn each of these into a set of vectors, so they operate in the same semantic space, and then have them all be searched with RAG, so we could return text, a chart, etc.\n",
    "\n",
    "Certain [tools](https://unstructured.io/?ref=blog.langchain.dev) can be used to split up documents in to these different embeddings.\n",
    "\n",
    "Think of it as a way to do multimodal RAG.\n",
    "\n",
    "Ref: https://blog.langchain.dev/semi-structured-multi-modal-rag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7251fd-cc5c-4bda-ac94-81fd7119c165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://blog.langchain.dev/content/images/2023/10/mvr_overview.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url=\"https://blog.langchain.dev/content/images/2023/10/mvr_overview.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f70697a-e477-4db1-bb21-c236e82e0ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
