{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca3c5ca-eb87-40ab-b45d-973627ac4be8",
   "metadata": {},
   "source": [
    "# Karpathy Notes\n",
    "\n",
    "Notes from Andrej Karpathy's AI playlist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19473b-052f-4b4e-9c90-d1a1a9beeb64",
   "metadata": {},
   "source": [
    "## Intro to LLMs\n",
    "\n",
    "[Video](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n",
    "\n",
    "### What are LLMs?\n",
    "\n",
    "LLMs essentially consist of 2 files:\n",
    "\n",
    "```\n",
    "llama-2-70b\n",
    "- parameters (140GB)\n",
    "- run.c (~500l of C code)\n",
    "```\n",
    "\n",
    "Let's break it down...\n",
    "\n",
    "* **llamba-2-70b**: Llama model, V2, 70B params\n",
    "* **parameters**: 2 bytes per param, so around 140GB total size, params of the neural net, fewer params mean faster model, but less accuract\n",
    "* **run.c**: Runs the neural net using the above params\n",
    "\n",
    "_Note: Latest models likely have 10X number of params_\n",
    "\n",
    "### Stage 1: Pre-training\n",
    "\n",
    "To get the paramters, we kind of \"compress\" the internet. Almost like creating a zip file of the internet.\n",
    "\n",
    "```\n",
    "\n",
    "| 10GB chunk of internet > 6000 GPUs, $2M, 12 days > 140GB parameter file.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "This would only be done maybe once per year.\n",
    "\n",
    "Now you have a neural network that can word prediction. Word prediction actually teaches us a lot about the world. E.g. given a birth date would could predict the death date based on the internet crawl data.\n",
    "\n",
    "![nn.png](./imgs/nn.jpg)\n",
    "\n",
    "#### Dreaming\n",
    "\n",
    "Now we have a model that can \"dream\" internet documents. We feed it a word, it can predict the next work, and repeat. For example, we could \"dream\" an amazon review, wiki entry etc.\n",
    "\n",
    "#### Transformer Architecture\n",
    "\n",
    "What does the neural net look like?\n",
    "It uses the transformer architecture.\n",
    "\n",
    "![nn.png](./imgs/transformer.jpg)\n",
    "\n",
    "* Billions of params dispersed throughout the network\n",
    "* We know how to improve them\n",
    "* But we don't really know how they collaborate together to do it\n",
    "\n",
    "### Stage 2: Fine Tuning (building an assistant)\n",
    "\n",
    "Instead of just creating dream docs, we want to build an assistant (i.e., be able to ask questions and get answers).\n",
    "\n",
    "To do this, we use fine tuning.\n",
    "\n",
    "We have a list of prompts or questions, and have humans fill in the answers. It's kind of like labeling questions with correct answers.\n",
    "\n",
    "We then use the training process again on this curated, high-quality data set.\n",
    "\n",
    "Now, when we ask questions to the model, it will reply using this question-answer format, but can still use data from pre-training.\n",
    "\n",
    "The actual answer generation may be done in collaboration between humans and machines, where machines suggest answers or part of answers.\n",
    "\n",
    "If the model has mistakes, they can be fed into this process.\n",
    "\n",
    "### Stage 3: RLHF (Optional)\n",
    "\n",
    "_RLHF: Reinforcement Learning with Human Feedback_\n",
    "\n",
    "To further refine the model, we can show users a list of possible responses and have them pick the best one. This is useful for areas where it's difficult for humans to create an answer. E.g., write a haiku about X.\n",
    "\n",
    "### Summary\n",
    "\n",
    "_Every Year..._\n",
    "\n",
    "__Pre-training__\n",
    "\n",
    "1. Download internet crawl\n",
    "2. Get a huge cluster of GPUs\n",
    "3. Compress into parameters, pay $2M, wait 12 days\n",
    "\n",
    "Obtained base model.\n",
    "\n",
    "_Every Week..._\n",
    "\n",
    "1. Write lableing instructions (for humans)\n",
    "2. Hire users to write 100k ideal Q&A responses / rank comparisons\n",
    "3. Finetune on this data\n",
    "\n",
    "Obtained assistant model.\n",
    "\n",
    "4. Run evaluations\n",
    "5. Deploy\n",
    "6. Collect misbehaviors, correct, feed into step 1 and repeat\n",
    "\n",
    "\n",
    "### Multi-modality\n",
    "\n",
    "Can interpret not just text, but videos, audio, etc.\n",
    "\n",
    "E.g., can draw website and have GPT generate the actual working code.\n",
    "\n",
    "### Custom GPTs\n",
    "\n",
    "Create your own GPT. This is not fine-tuning, but rather you just provide it with:\n",
    "\n",
    "1. Predefined instruction\n",
    "2. Which tools it can access\n",
    "3. Files that can be searched with RAG\n",
    "\n",
    "### Future Direction\n",
    "\n",
    "* LLM performance so far is a predictible function of\n",
    "    * N: Num of params\n",
    "    * D: Amount of text trained on\n",
    "    \n",
    "* So we can expect performance to improve\n",
    "* Possible we may think of more _system 2_ (\"slow\") thinking\n",
    "* Self-improvement, similar to RL\n",
    "* LLM like a central kernel for problem solving -- reads text, browses the internet, calls the appropriate tools to problem solve, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9196e28f-046f-4867-9ecd-54721363c35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
